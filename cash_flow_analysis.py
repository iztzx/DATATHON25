import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Set up AZ color scheme
AZ_COLORS = {
    'mulberry': '#830051',
    'lime_green': '#C4D600',
    'navy': '#003865',
    'graphite': '#3F4444',
    'light_blue': '#68D2DF',
    'magenta': '#D0006F',
    'purple': '#3C1053',
    'gold': '#F0AB00'
}

class CashFlowAnalyzer:
    def __init__(self, dataset_path):
        """Initialize the Cash Flow Analyzer with dataset path."""
        self.dataset_path = dataset_path
        self.df = None
        self.weekly_data = None
        self.forecasts = {}
        self.anomalies = None
        
    def load_data(self):
        """Load the PRE-CLEANED cash flow dataset (CSV)."""
        print("Loading cleaned dataset...")
        try:
            # Load the CSV generated by clean_data.py
            schema = {
                'Amount in USD': float,
                'Net_Amount_USD': float,
                'Week_Num': int,
                'Year': int
            }
            # Low_memory=False to handle mixed types if any, though schema helps
            self.df = pd.read_csv('AstraZeneca_Cleaned_Processed_Data.csv', dtype=schema, parse_dates=['posting_date', 'week'])
            
            print(f"Cleaned Dataset loaded. Shape: {self.df.shape}")
            
            # Verify critical columns exist (The "Theory" inputs)
            required_cols = ['Net_Amount_USD', 'Activity', 'date', 'Category']
            # Note: 'date' might be 'posting_date' in CSV
            if 'posting_date' in self.df.columns:
                self.df['date'] = self.df['posting_date']
            
            
            # PURE CSV Strategy: No external Excel dependencies
            # We will derive "Cumulative Position" from Flows if needed, 
            # rather than relying on a potentially disconnected Balance sheet.
            self.df_balance = None 
            self.df_cat_link = None 
            self.df_country = None
            
            return True
        except FileNotFoundError:
            print("Error: 'AstraZeneca_Cleaned_Processed_Data.csv' not found. Run clean_data.py first!")
            return False
        except Exception as e:
            print(f"Error loading dataset: {e}")
            return False
    
    def explore_data(self):
        """Explore and understand the dataset structure."""
        print("\n=== DATA EXPLORATION ===")
        
        # Basic info
        print(f"Dataset shape: {self.df.shape}")
        if 'Name' in self.df.columns:
            print(f"Number of unique entities: {self.df['Name'].nunique()}")
        if 'Pstng Date' in self.df.columns:
            print(f"Date range: {self.df['Pstng Date'].min()} to {self.df['Pstng Date'].max()}")
        
        # Cash flow categories
        if 'Category' in self.df.columns:
            print(f"\nCash flow categories:")
            category_counts = self.df['Category'].value_counts().head(10)
            print(category_counts)
        
        # Currency distribution
        if 'Curr.' in self.df.columns:
            print(f"\nCurrency distribution:")
            currency_counts = self.df['Curr.'].value_counts()
            print(currency_counts)
        
        # Basic statistics for amounts
        if 'Amount in doc. curr.' in self.df.columns:
            print(f"\nAmount statistics (in document currency):")
            amount_stats = self.df['Amount in doc. curr.'].describe()
            print(amount_stats)
        
        return True
    
    def preprocess_data(self):
        """Preprocess data using pre-cleaned attributes (CSV Source)."""
        print("\n=== DATA PREPROCESSING (Optimized) ===")
        
        # 1. Verify Activity Column (Crucial for Theory)
        if 'Activity' in self.df.columns:
             print("  • Activity column verified (Operating/Investing/Financing).")
        else:
             print("  • Warning: 'Activity' column missing. Check clean_data.py.")

        # 2. Date/Time checks
        # 'month' might be missing if only 'week' was in CSV or not parsed as date
        if 'month' not in self.df.columns and 'posting_date' in self.df.columns:
            self.df['month'] = self.df['posting_date'].dt.to_period('M').dt.start_time
            
        # 3. Aggregation (Weekly)
        # We group by Week, Category, Activity to maintain granularity for Forecasts
        group_cols = ['week']
        if 'Category' in self.df.columns:
            group_cols.append('Category')
        if 'Activity' in self.df.columns:
            group_cols.append('Activity') # Critical for DSS
            
        # Use 'Net_Amount_USD' if possible (the "Theoretically Correct" signed amount)
        # Fallback to 'Amount in USD' (which might be absolute)
        val_col = 'Amount in USD'
        if 'Net_Amount_USD' in self.df.columns:
             val_col = 'Net_Amount_USD'
        else:
             print("  • Note: Net_Amount_USD not found, using Amount in USD.")
        
        # Aggregation Dictionary
        agg_dict = {
            val_col: 'sum',
        }
        
        self.weekly_data = self.df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Rename for internal consistency with Forecast engine
        self.weekly_data = self.weekly_data.rename(columns={
            val_col: 'weekly_amount_usd'
        })
        
        # Sort
        self.weekly_data = self.weekly_data.sort_values('week')
        
        print(f"Weekly data aggregated. Records: {len(self.weekly_data)}")
        return True

    def preprocess_data_deprecated(self):
        """Preprocess data for analysis using Pandas."""
        print("\n=== DATA PREPROCESSING ===")
        
        # 1. Merge Category Linkage (Operating / Investing / Financing)
        if self.df_cat_link is not None and 'Category' in self.df.columns:
            # Assume Linkage sheet has 'Category' and 'Cash Flow Type' or similar
            # Let's check columns for robustness (Hardcoding based on standard template expectation)
            # Typically: 'Category', 'Activity' or 'Type'
            print("Merging Category Linkage...")
            
            # Standardize names for merge
            if 'Category' in self.df_cat_link.columns:
                # Normalize strings (strip whitespace, consistent case)
                self.df['Category_Clean'] = self.df['Category'].astype(str).str.strip()
                self.df_cat_link['Category_Clean'] = self.df_cat_link['Category'].astype(str).str.strip()
                
                self.df = pd.merge(self.df, self.df_cat_link, left_on='Category_Clean', right_on='Category_Clean', how='left', suffixes=('', '_link'))
                
                # Identify the activity column (usually the 2nd column if not named standardly)
                # We rename it to 'Activity' for consistency
                new_cols = [c for c in self.df.columns if c not in self.df_cat_link.columns or c == 'Category']
                added_cols = [c for c in self.df.columns if c not in new_cols]
                
                if added_cols:
                    activity_col = added_cols[0] # Take the first new column as Activity
                    self.df['Activity'] = self.df[activity_col]
                    print(f"Mapped Categories to Activity using column: {activity_col}")
            else:
                 print("Warning: 'Category' column not found in Linkage sheet.")

        # 2. Merge Country Mapping (if useful later)
        if self.df_country is not None and 'Name' in self.df.columns:
             pass # Logic for country merge if needed, skipping for now to focus on Cash Flow

        # Convert posting date to datetime
        if 'Pstng Date' in self.df.columns:
            self.df['posting_date'] = pd.to_datetime(self.df['Pstng Date'], errors='coerce')
            # Drop rows with invalid dates
            self.df = self.df.dropna(subset=['posting_date'])
        
        # Add week and month columns for time series analysis
        self.df['week'] = self.df['posting_date'].dt.to_period('W').dt.start_time
        self.df['month'] = self.df['posting_date'].dt.to_period('M').dt.start_time
        
        # Create cash flow direction (inflow/outflow)
        if 'Amount in doc. curr.' in self.df.columns:
            self.df['cash_flow_direction'] = np.where(
                self.df['Amount in doc. curr.'] > 0, 'Inflow', 'Outflow'
            )
        
        # Aggregate to weekly level
        group_cols = ['week']
        if 'Category' in self.df.columns:
            group_cols.append('Category')
        if 'Activity' in self.df.columns:
            group_cols.append('Activity') # Group by Activity too
            
        group_cols.append('cash_flow_direction')
        
        agg_dict = {}
        if 'Amount in doc. curr.' in self.df.columns:
            agg_dict['Amount in doc. curr.'] = 'sum'
        if 'Amount in USD' in self.df.columns:
            agg_dict['Amount in USD'] = 'sum'
        if 'DocumentNo' in self.df.columns:
            agg_dict['DocumentNo'] = 'count'
        
        self.weekly_data = self.df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Rename columns for clarity
        if 'Amount in doc. curr.' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'Amount in doc. curr.': 'weekly_amount'})
        if 'Amount in USD' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'Amount in USD': 'weekly_amount_usd'})
        if 'DocumentNo' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'DocumentNo': 'transaction_count'})
        
        # Sort by week
        self.weekly_data = self.weekly_data.sort_values('week')
        
        # --- EXPORT CLEANED DATA ---
        export_path = 'AstraZeneca_Cleaned_Processed_Data.csv'
        print(f"\n[EXPORTing] Saving cleaned dataset to '{export_path}'...")
        # Select key columns for the user
        export_cols = [c for c in self.df.columns if c in [
            'Name', 'DocumentNo', 'posting_date', 'week', 'Category', 'Category_Clean', 
            'Activity', 'Amount in doc. curr.', 'Amount in USD', 'cash_flow_direction'
        ]]
        self.df[export_cols].to_csv(export_path, index=False)
        print(f"  • Export complete. Rows: {len(self.df)}")
        
        print(f"Weekly data created with {len(self.weekly_data)} records")
        return True
    
    def _generate_forecast_model(self, series, name="Total"):
        """
        Generate forecast model for a given time series.
        Returns dictionary with forecast data and metrics.
        """
        # Guard clause for empty series
        if series.empty:
            print(f"Warning: No data for {name}. Returning empty forecast.")
            empty_series = pd.Series([], dtype=float)
            return {
                'name': name,
                '1month': empty_series,
                '6month': empty_series,
                'historical': empty_series,
                'es_values': empty_series,
                'trend': 0,
                'mae': 0,
                'rmse': 0
            }

        # Calculate trend using linear regression on recent data
        recent_weeks = min(12, len(series))
        
        
        # Optimize parameters (Cleaned up duplications)
        alpha, beta = self._optimize_holt_parameters(series)
        
        try:
            level = series.iloc[0]
        except IndexError:
            print(f"\nCRITICAL ERROR in _generate_forecast_model for '{name}':")
            print(f"  Type: {type(series)}")
            print(f"  Shape: {series.shape}")
            print(f"  Empty: {series.empty}")
            print(f"  Head: {series.head()}")
            # Return empty to avoid crash
            return {
                'name': name,
                '1month': pd.Series([], dtype=float),
                '6month': pd.Series([], dtype=float),
                'historical': series,
                'es_values': pd.Series([], dtype=float),
                'trend': 0, 'mae': 0, 'rmse': 0
            }
            
        trend = 0
        
        es_values = []
        
        for value in series.values:
            new_level = alpha * value + (1 - alpha) * (level + trend)
            new_trend = beta * (new_level - level) + (1 - beta) * trend
            es_values.append(new_level)
            level = new_level
            trend = new_trend
            
        last_date = series.index[-1]
        last_level = level
        last_trend = trend
        
        # 1-month forecast (4 weeks)
        forecast_1month_dates = pd.date_range(start=last_date + timedelta(weeks=1), periods=4, freq='W')
        forecast_1month_values = []
        historical_volatility = series.std() if len(series) > 1 else 0
        
        for i in range(4):
            trended_value = last_level + (i + 1) * last_trend
            variation = np.random.normal(0, historical_volatility * 0.3)
            forecast_1month_values.append(trended_value + variation)
            
        forecast_1month = pd.Series(forecast_1month_values, index=forecast_1month_dates)
        
        # 6-month forecast (24 weeks)
        forecast_6month_dates = pd.date_range(start=last_date + timedelta(weeks=1), periods=24, freq='W')
        forecast_6month_values = []
        damping_factor = 0.95
        
        for i in range(24):
            damped_trend = last_trend * (damping_factor ** i)
            trended_value = last_level + (i + 1) * damped_trend
            variation_std = historical_volatility * 0.3 * (1 - i/24)
            variation = np.random.normal(0, variation_std)
            forecast_6month_values.append(trended_value + variation)
            
        forecast_6month = pd.Series(forecast_6month_values, index=forecast_6month_dates)
        
        # Accuracy metrics
        mae, rmse = 0, 0
        if len(series) > 8:
            test_actual = series.iloc[-8:]
            # Simple moving average as baseline for error calculation
            ma_baseline = series.rolling(window=4).mean().shift(1)
            test_forecast = ma_baseline.iloc[-8:]
            
            # Align
            common_idx = test_actual.index.intersection(test_forecast.index)
            if len(common_idx) > 0:
                mae = mean_absolute_error(test_actual[common_idx], test_forecast[common_idx])
                rmse = np.sqrt(mean_squared_error(test_actual[common_idx], test_forecast[common_idx]))

        return {
            'name': name,
            '1month': forecast_1month,
            '6month': forecast_6month,
            'historical': series,
            'es_values': pd.Series(es_values, index=series.index),
            'trend': last_trend,
            'mae': mae,
            'rmse': rmse
        }

    def create_forecasts(self):
        """Create time series forecasts for cash flow, activities, and ending balance."""
        print("\n=== TIME SERIES FORECASTING & CLOSING BALANCE ===")
        
        # 1. Total Cash Flow Forecast
        weekly_totals = self.weekly_data.groupby('week')['weekly_amount_usd'].sum()
        print("Generating forecast for Total Net Cash Flow...")
        self.forecasts['total'] = self._generate_forecast_model(weekly_totals, "Total Net Cash Flow")
        
        # Backward compatibility
        self.forecasts['historical'] = self.forecasts['total']['historical']
        self.forecasts['1month'] = self.forecasts['total']['1month']
        self.forecasts['6month'] = self.forecasts['total']['6month']
        self.forecasts['es_values'] = self.forecasts['total']['es_values']
        
        # 2. Activity-Based Forecasts (Operating / Investing / Financing)
        print("Generating forecasts by Activity (Operating, Investing, Financing)...")
        self.forecasts['activities'] = {}
        if 'Activity' in self.weekly_data.columns:
            activities = self.weekly_data['Activity'].unique()
            for act in activities:
                # Need to handle NaN activity
                if pd.isna(act): continue
                
                print(f"  - Forecasting for: {act}")
                act_data = self.weekly_data[self.weekly_data['Activity'] == act]
                act_weekly = act_data.groupby('week')['weekly_amount_usd'].sum()
                act_weekly = act_weekly.reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['activities'][str(act)] = self._generate_forecast_model(act_weekly, str(act))
                
        # 3. Forecast Ending Cash Balance
        # We need the LAST ACTUAL closing balance from self.df_balance
        print("Projecting Ending Cash Balances...")
        self.forecasts['balance'] = {}
        
        # 3. Forecast Cumulative Net Position (Relative Liquidity)
        # Since we removed external Balance sheet, we track Cumulative Flow Trend
        print("Projecting Cumulative Net Cash Position...")
        self.forecasts['balance'] = {}
        
        # Start from 0 (Relative Change)
        last_balance = 0
        self.forecasts['balance']['last_actual'] = last_balance # Proxy
        
        # 1-Month Cumulative Forecast
        fc_1m_flows = self.forecasts['total']['1month']
        fc_1m_bal = []
        running_bal = last_balance
        for flow in fc_1m_flows.values:
            running_bal += flow
            fc_1m_bal.append(running_bal)
        self.forecasts['balance']['1month'] = pd.Series(fc_1m_bal, index=fc_1m_flows.index)
        
        # 6-Month Cumulative Forecast
        fc_6m_flows = self.forecasts['total']['6month']
        fc_6m_bal = []
        running_bal = last_balance
        for flow in fc_6m_flows.values:
            running_bal += flow
            fc_6m_bal.append(running_bal)
        self.forecasts['balance']['6month'] = pd.Series(fc_6m_bal, index=fc_6m_flows.index)
        
        print("  - Generated Relative Liquidity Projection (Cumulative Flow)")

        # 4. Category Forecasts (Top Drivers)
        print("Generating forecasts for Top Categories...")
        self.forecasts['categories'] = {}
        
        if 'Category' in self.weekly_data.columns:
            # Identify top 2 categories by volume
            cat_volumes = self.weekly_data.groupby('Category')['weekly_amount_usd'].apply(lambda x: x.abs().sum())
            top_categories = cat_volumes.nlargest(2).index.tolist()
            
            for cat in top_categories:
                print(f"  - Forecasting for Category: {cat}")
                cat_data = self.weekly_data[self.weekly_data['Category'] == cat]
                cat_weekly = cat_data.groupby('week')['weekly_amount_usd'].sum()
                # Reindex
                cat_weekly = cat_weekly.reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['categories'][cat] = self._generate_forecast_model(cat_weekly, cat)

        # 5. Entity Forecasts (Top Spenders) - [NEW] Extracting More Value
        print("Generating forecasts for Top Entities...")
        self.forecasts['entities'] = {}
        
        if 'Name' in self.df.columns:
            # Aggregate weekly by Name
            # We need to rebuild weekly agg for Name since self.weekly_data might not have it grouped
            # Let's check self.weekly_data or go back to self.df
            # self.weekly_data grouped by [week, Category, Activity]. Name is lost.
            # Go back to self.df
            
            # Top 5 Spenders (Outflow)
            outflows = self.df[self.df['Amount in USD'] < 0]
            top_entities = outflows.groupby('Name')['Amount in USD'].sum().abs().nlargest(5).index.tolist()
            
            for ent in top_entities:
                print(f"  - Forecasting for Entity: {ent}")
                # Filter and Agg
                ent_data = self.df[self.df['Name'] == ent]
                if 'week' not in ent_data.columns:
                     ent_data['week'] = pd.to_datetime(ent_data['posting_date']).dt.to_period('W').dt.start_time
                
                ent_weekly = ent_data.groupby('week')['Amount in USD'].sum()
                ent_weekly = ent_weekly.sort_index().reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['entities'][ent] = self._generate_forecast_model(ent_weekly, ent)

        # 5. EXPORT FORECAST DATA (ESS Ready)
        print("Exporting Forecast Results to CSV...")
        forecast_rows = []
        
        # Helper to unpack forecast object
        def unpack_forecast(model_out, fc_type):
            # 1-Month Forecast
            if '1month' in model_out and not model_out['1month'].empty:
                for date, val in model_out['1month'].items():
                    forecast_rows.append({
                        'Forecast_Type': fc_type,
                        'Model_Name': model_out['name'],
                        'Date': date,
                        'Value_USD': val,
                        'Scenario': 'Base Case',
                        'Horizon': 'Short-Term (1M)'
                    })
            # 6-Month Forecast
            if '6month' in model_out and not model_out['6month'].empty:
                for date, val in model_out['6month'].items():
                    forecast_rows.append({
                        'Forecast_Type': fc_type,
                        'Model_Name': model_out['name'],
                        'Date': date,
                        'Value_USD': val,
                        'Scenario': 'Base Case',
                        'Horizon': 'Medium-Term (6M)'
                    })
        
        # Unpack Total
        if 'total' in self.forecasts:
            unpack_forecast(self.forecasts['total'], 'Total Net Flow')
            
        # Unpack Activities
        if 'activities' in self.forecasts:
            for act_name, model in self.forecasts['activities'].items():
                unpack_forecast(model, f"Activity: {act_name}")
                
        # Unpack Categories
        if 'categories' in self.forecasts:
            for cat_name, model in self.forecasts['categories'].items():
                unpack_forecast(model, f"Category: {cat_name}")
                
        # Save to CSV
        if forecast_rows:
            pd.DataFrame(forecast_rows).to_csv('AstraZeneca_Forecast_Results.csv', index=False)
            print(f"  • Forecasts exported: {len(forecast_rows)} rows")
                
        return True

    def answer_suggested_questions(self):
        """
        Generate a Decision Support System (DSS) Executive Report.
        Answers AstraZeneca's key questions with data-driven strategic insights.
        """
        print("\n" + "#"*70)
        print("   ASTRAZENECA EXECUTIVE DECISION SUPPORT SYSTEM (DSS) REPORT")
        print("   CLASSIFICATION: STRICTLY CONFIDENTIAL / COMPANY RESTRICTED")
        print("#"*70)
        
        # ---------------------------------------------------------
        # SECTION 1: Strategic Forecast (1-Month & 6-Month)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 1. LIQUIDITY FORECASTING & STRATEGY")
        print("="*70)
        
        total_fc = self.forecasts['total']
        is_growth = total_fc['trend'] > 0
        trend_status = "POSITIVE GROWTH" if is_growth else "CONTRACTION ALERT"
        
        print(f"\n[SHORT-TERM] 1-Month Outlook: {trend_status}")
        print(f"  • Expected Net Position (4-Week Sum): ${total_fc['1month'].sum()/1e6:.2f}M")
        print(f"  • Weekly Trend Slope: ${total_fc['trend']/1e6:.2f}M per week")
        print(f"  • Model Confidence (RMSE): +/- ${total_fc['rmse']/1e6:.2f}M")
        
        print(f"\n[MEDIUM-TERM] 6-Month Trajectory")
        if not total_fc['6month'].empty:
            end_bal = total_fc['6month'].iloc[-1]
            print(f"  • Projected Weekly Flow by Month 6: ${end_bal/1e6:.2f}M")
            print(f"  • Sustainability Score: {'High' if end_bal > 0 else 'Medium-Risk'}")
        else:
            print("  • Projection data unavailable (insufficient history)")
        
        print("\n>>> DECISION SUPPORT: RECOMMENDED ACTIONS")
        if total_fc['1month'].sum() < 0:
             print("  [ACTION] TRIGGER LIQUIDITY CONTINGENCY: Short-term flows are projected negative.")
             print("  [ACTION] REVIEW: Delay discretionary payments scheduled for weeks 3-4.")
        else:
             print("  [ACTION] INVEST SURPLUS: Excess liquidity identified. Evaluate short-term investment instruments.")
             
        # ---------------------------------------------------------
        # SECTION 2: Anomaly Triage (Risk & Control)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 2. RISK & CONTROL: ANOMALY TRIAGE")
        print("="*70)
        
        if self.anomalies is not None and not self.anomalies.empty:
            print(f"\n[ALERT] {len(self.anomalies)} Transactions Flagged for Review")
            
            # Group by type
            summary = self.anomalies['anomaly_type'].value_counts()
            for atype, count in summary.items():
                print(f"  • {atype}: {count} items")
            
            print("\n>>> HIGH PRIORITY INVESTIGATION LIST (Top Risks)")
            high_risk = self.anomalies.head(5)
            print(f"{'Date':<12} | {'DocNo':<10} | {'Type':<20} | {'Amount ($)':>12} | {'Category'}")
            print("-" * 80)
            
            for idx, row in high_risk.iterrows():
                d = str(row['posting_date'].date()) if 'posting_date' in row else 'N/A'
                doc = str(row.get('DocumentNo', 'N/A'))
                atype = str(row.get('anomaly_type', 'Unknown'))
                amt = f"{row.get('Amount in USD', 0):,.2f}"
                cat = str(row.get('Category', 'N/A'))[:20]
                print(f"{d:<12} | {doc:<10} | {atype:<20} | {amt:>12} | {cat}")
                
            print("\n>>> DECISION SUPPORT: INVESTIGATION PROTOCOL")
            print("  [ACTION - DUPLICATES]: Verify if 'Potential Duplicates' share Invoice References in source system.")
            print("  [ACTION - ROUND NUMBERS]: Request supporting documentation for large round-number manual entries.")
            print("  [ACTION - SPIKES]: Confirm if statistical outliers align with known strategic initiatives (M&A, Capex).")
        else:
            print("\n[STATUS] No material anomalies detected. Standard monitoring active.")

        # ---------------------------------------------------------
        # SECTION 3: Driver Analysis (Business Logic)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 3. STRATEGIC FINANCIAL METRICS & DRIVERS")
        print("="*70)
        
        # A. CALCULATE METRICS (More Theory)
        # Filter for Operating Activity
        if 'Activity' in self.weekly_data.columns:
            op_data = self.weekly_data[self.weekly_data['Activity'] == 'Operating']
            
            # 1. Cash Burn Rate (Avg Weekly Outflow)
            # Filter where amount < 0
            op_outflows = op_data[op_data['weekly_amount_usd'] < 0]['weekly_amount_usd']
            avg_burn_weekly = op_outflows.mean() if not op_outflows.empty else 0
            
            # 2. Operating Efficiency Ratio (Inflow / Outflow)
            op_inflows = op_data[op_data['weekly_amount_usd'] > 0]['weekly_amount_usd'].sum()
            op_out_total = op_data[op_data['weekly_amount_usd'] < 0]['weekly_amount_usd'].abs().sum()
            efficiency_ratio = op_inflows / op_out_total if op_out_total != 0 else 0
            
            # 3. Liquidity Coverage (Runway)
            last_bal = self.forecasts['balance']['last_actual'] if 'balance' in self.forecasts and 'last_actual' in self.forecasts['balance'] else 0
            runway_weeks = (last_bal / abs(avg_burn_weekly)) if avg_burn_weekly != 0 else 999
            
            print(f"\n[KEY PERFORMANCE INDICATORS (KPIs)]")
            print(f"  • Operating Cash Burn: ${abs(avg_burn_weekly)/1e6:.2f}M / week")
            print(f"  • Operating Efficiency: {efficiency_ratio:.2f}x (Target > 1.0)")
            print(f"    (For every $1 out, company generates ${efficiency_ratio:.2f})")
            
            if runway_weeks < 12:
                 print(f"  • RUNWAY ALERT: Cash balance covers only {runway_weeks:.1f} weeks of operating burn.")
            else:
                 print(f"  • Liquidity Health: robust coverage detected ({runway_weeks:.1f} weeks runway).")
                 
        if 'categories' in self.forecasts:
            print("\n[KEY CATEGORY DRIVERS]")
            for cat, data in self.forecasts['categories'].items():
                start_val = data['historical'].tail(4).mean()
                end_val = data['1month'].mean()
                pct_change = ((end_val - start_val) / start_val) * 100 if start_val != 0 else 0
                
                direction = "IMPROVING" if (end_val > start_val and end_val > 0) else "DECLINING"
                print(f"  • {cat}: {direction} ({pct_change:+.1f}%) -> Avg Next Month: ${end_val/1e6:.1f}M")

        print("\n[METHODOLOGY NOTE]")
        print("  • Model: Optimized Holt-Winters Exponential Smoothing (Auto-Tuned Alpha/Beta).")
        print("  • Damping: applied to long-term forecasts to prevent variance explosion.")
        print("  • Scenarios: Volatility-adjusted simulations included in visual dashboard.")
        print("#"*70 + "\n")
        
        return True

    
    def _optimize_holt_parameters(self, series):
        """
        Grid search to find optimal alpha (level) and beta (trend) parameters.
        Returns best params and the associated error.
        """
        best_alpha, best_beta = 0.3, 0.2
        best_rmse = float('inf')
        
        # Grid search range
        alphas = [0.1, 0.3, 0.5, 0.7, 0.9]
        betas = [0.1, 0.2, 0.3, 0.4]
        
        # Split for validation (last 4 weeks as validation set)
        if len(series) < 8:
            return best_alpha, best_beta
            
        train = series.iloc[:-4]
        valid = series.iloc[-4:]
        
        if train.empty:
            return best_alpha, best_beta
            
        try:
           # Dry run to check index access
           _ = train.iloc[0]
        except:
           return best_alpha, best_beta
        
        for a in alphas:
            for b in betas:
                # Run Holt's on train
                level = train.iloc[0]
                trend = 0
                preds = []
                
                # Fit
                for val in train.values:
                    last_level = level
                    level = a * val + (1 - a) * (last_level + trend)
                    trend = b * (level - last_level) + (1 - b) * trend
                
                # Forecast
                last_level = level
                last_trend = trend
                for i in range(4):
                     preds.append(last_level + (i+1)*last_trend)
                
                # Evaluate
                try:
                    rmse = np.sqrt(mean_squared_error(valid, preds))
                    if rmse < best_rmse:
                        best_rmse = rmse
                        best_alpha = a
                        best_beta = b
                except:
                    continue
                    
        return best_alpha, best_beta

    def detect_anomalies(self):
        """
        Detect anomalies using a Hybrid approach: 
        1. Statistical (Isolation Forest)
        2. Rule-based (Accounting irregularities)
        """
        print("\n=== HYBRID ANOMALY DETECTION (DSS MODULE) ===")
        
        # 1. Statistical Detection (Isolation Forest)
        # ---------------------------------------------
        feature_cols = []
        if 'Amount in doc. curr.' in self.df.columns:
            feature_cols.append('Amount in doc. curr.')
        if 'Amount in USD' in self.df.columns:
            feature_cols.append('Amount in USD')
        
        features = self.df[feature_cols].copy()
        
        if 'Category' in self.df.columns:
            category_dummies = pd.get_dummies(self.df['Category'], prefix='category')
            features = pd.concat([features, category_dummies], axis=1)
        
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features.fillna(0))
        
        iso_forest = IsolationForest(contamination=0.03, random_state=42) # Reduced to 3% to focus on high priority
        stat_anomaly_mask = iso_forest.fit_predict(features_scaled) == -1
        
        # Initialize anomaly column
        self.df['anomaly_type'] = None
        self.df.loc[stat_anomaly_mask, 'anomaly_type'] = 'Statistical Outlier'
        
        # 2. Rule-Based Detection (Accounting Logic)
        # ---------------------------------------------
        
        # Rule A: Potential Duplicates (Same Amount, Same Date, Different Doc)
        print("Scaning for duplicate payments...")
        if all(col in self.df.columns for col in ['Amount in USD', 'posting_date', 'Category']):
            # Find duplicates based on Amount, Date, Category (ignoring DocumentNo)
            # We filter for non-zero amounts
            mask_real = self.df['Amount in USD'].abs() > 0.01
            
            dupe_cols = ['Amount in USD', 'posting_date', 'Category']
            duplicates = self.df[mask_real].duplicated(subset=dupe_cols, keep=False)
            
            # Update anomaly type (prioritize specific rules over statistical)
            self.df.loc[duplicates & mask_real, 'anomaly_type'] = 'Potential Duplicate'

        # Rule B: Round Number Checks (e.g. 100000.00) - often manual
        print("Scanning for 'Round Number' manual entries...")
        if 'Amount in USD' in self.df.columns:
            # Check if amount is round thousand
            is_round = (self.df['Amount in USD'].abs() % 1000 == 0) & (self.df['Amount in USD'].abs() > 1000)
            self.df.loc[is_round & (self.df['anomaly_type'].isna()), 'anomaly_type'] = 'Round Number (Manual Risk)'

        # Rule C: Weekend Postings (Unusual for corporate)
        print("Scanning for Weekend transactions...")
        if 'posting_date' in self.df.columns:
            is_weekend = self.df['posting_date'].dt.dayofweek >= 5
            # Only trigger if not already flagged
            self.df.loc[is_weekend & (self.df['anomaly_type'].isna()), 'anomaly_type'] = 'Weekend Posting'

        # Consolidate
        self.anomalies = self.df[self.df['anomaly_type'].notna()].copy()
        
        # Prioritize anomalies (Risk Scoring)
        risk_map = {
            'Potential Duplicate': 3,   # High
            'Statistical Outlier': 2,   # Medium
            'Weekend Posting': 1,       # Low risk but notable
            'Round Number (Manual Risk)': 1
        }
        self.anomalies['risk_score'] = self.anomalies['anomaly_type'].map(risk_map)
        self.anomalies = self.anomalies.sort_values(by=['risk_score', 'Amount in USD'], ascending=[False, False])
        
        count = len(self.anomalies)
        print(f"DSS Alert: Detected {count} irregularities requiring review.")
        print(f" Breakdown: {self.anomalies['anomaly_type'].value_counts().to_dict()}")
        
        return True
    
    def generate_visualizations(self):
        """
        Generate Best-Practice Static Dashboard (V3).


        aligned with Interactive Command Center visuals.
        """
        print("\n=== GENERATING STATIC DASHBOARD (V3) ===")
        # Set style (try seaborn, fallback to ggplot)
        import matplotlib.dates as dates
        try:
            plt.style.use('seaborn-v0_8-darkgrid')
        except:
            plt.style.use('ggplot')
            
        # STRICT AZ BRAND COLORS (From Image)
        AZ_COLORS = {
            'magenta': '#D0006F',       # Primary 1
            'mulberry': '#830051',      # Primary 2
            'lime_green': '#C4D600',    # Primary 3
            'gold': '#F0AB00',          # Primary 4 (Darkened for readability)
            'navy': '#003865',          # Text / Strong Elements
            'platinum': '#EBEFEE',      # Background Light
            'off_white': '#F8F8F8',     # Background Lighter
            'graphite': '#3F4444',      # Text Secondary
            'support_blue': '#68D2DF',  # Supporting Cyan
            'rich_green': '#006F3D'     # Darker Green for positive distinctness
        }
        
        # 3 Rows x 2 Columns Layout
        fig, axes = plt.subplots(3, 2, figsize=(20, 18), facecolor=AZ_COLORS['off_white'])
        plt.suptitle("AstraZeneca Executive Cash Flow Dashboard (V3 - Brand Aligned)", fontsize=24, fontweight='bold', color=AZ_COLORS['navy'])
        plt.subplots_adjust(hspace=0.4, wspace=0.25)
        
        # Helper for currency formatting
        def currency_format(x, pos):
            if x >= 1e6:
                return f'${x*1e-6:.1f}M'
            elif x >= 1e3:
                return f'${x*1e-3:.0f}K'
            elif x <= -1e6:
                return f'-${abs(x)*1e-6:.1f}M'
            elif x <= -1e3:
                return f'-${abs(x)*1e-3:.0f}K'
            else:
                return f'${x:.0f}'
        
        from matplotlib.ticker import FuncFormatter
        currency_fmt = FuncFormatter(currency_format)

        # Helper for common styling
        def style_ax(ax, title):
            ax.set_title(title, fontweight='bold', fontsize=12, color=AZ_COLORS['navy'])
            ax.set_facecolor(AZ_COLORS['off_white'])
            ax.grid(True, axis='y', color='#e0e0e0', linestyle='--', linewidth=0.5)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['left'].set_color(AZ_COLORS['graphite'])
            ax.spines['bottom'].set_color(AZ_COLORS['graphite'])
            ax.tick_params(colors=AZ_COLORS['graphite'], labelsize=9)
            ax.yaxis.set_major_formatter(currency_fmt)

        # --- P1: LIQUIDITY FORECAST - UNIFIED FAN CHART (Top Left) ---
        ax1 = axes[0, 0]
        style_ax(ax1, "Liquidity Forecast (Unified Trend + Confidence)")
        
        # 1. Historical Net Flow (Line instead of Bars for continuity)
        weekly_net = self.weekly_data.groupby('week')['weekly_amount_usd'].sum()
        history = weekly_net.tail(16) # Show a bit more history
        
        # Plot History
        ax1.plot(history.index, history.values, color=AZ_COLORS['navy'], linewidth=2, label='Historical Net Flow')
        
        # 2. Forecast Data (Fan Chart)
        if 'total' in self.forecasts:
             fc_model = self.forecasts['total']
             # Combine 1m and 6m for plotting
             fc_series = pd.concat([fc_model['1month'], fc_model['6month']])
             # Remove duplicates if overlaps
             fc_series = fc_series[~fc_series.index.duplicated(keep='first')]
             
             # Connect history to forecast
             # Add the last history point to forecast to close the visual gap
             last_hist_date = history.index[-1]
             last_hist_val = history.values[-1]
             
             # Plots
             ax1.plot(fc_series.index, fc_series.values, color=AZ_COLORS['navy'], linestyle='--', linewidth=2, label='Forecast')
             
             # Confidence Interval (Fan) - Tightened and Faded
             rmse = fc_model['rmse']
             # Widen the cone over time but LESS aggressively (Tighten)
             upper_bound = []
             lower_bound = []
             for i, val in enumerate(fc_series.values):
                 # Reduced width multiplier from 0.1 to 0.05
                 uncertainty = rmse * (0.8 + 0.05 * i)
                 upper_bound.append(val + uncertainty)
                 lower_bound.append(val - uncertainty)
                 
             # FADED: Alpha reduced to 0.15
             ax1.fill_between(fc_series.index, lower_bound, upper_bound, color=AZ_COLORS['support_blue'], alpha=0.15, label='Confidence Interval')
             
             # Forecast Start Line
             ax1.axvline(x=last_hist_date, color=AZ_COLORS['gold'], linestyle=':', linewidth=1.5)
             ax1.text(last_hist_date, ax1.get_ylim()[1]*0.9, " FORECAST START", color=AZ_COLORS['gold'], fontsize=8, fontweight='bold')
             
        ax1.set_ylabel("Net Cash Flow ($)", color=AZ_COLORS['graphite'])
        ax1.tick_params(axis='x', rotation=45, labelsize=8)
        ax1.legend(loc='upper left', frameon=False, fontsize=8) 
        
        # --- P2: OPERATIONAL EFFICIENCY - DIFFERENCE GAP (Top Right) ---
        ax2 = axes[0, 1]
        style_ax(ax2, "Efficiency Gap (Inflow vs Outflow)")
        
        inflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] > 0].groupby('week')['weekly_amount_usd'].sum()
        outflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] < 0].groupby('week')['weekly_amount_usd'].sum().abs()
        
        all_weeks = inflow.index.union(outflow.index)
        # Smoothing (4W MA) for cleaner visuals
        inflow_ma = inflow.reindex(all_weeks, fill_value=0).rolling(window=4).mean()
        outflow_ma = outflow.reindex(all_weeks, fill_value=0).rolling(window=4).mean()
        
        # Plot Lines
        ax2.plot(inflow_ma.index, inflow_ma.values, color=AZ_COLORS['rich_green'], linewidth=2, label='Inflow')
        ax2.plot(outflow_ma.index, outflow_ma.values, color=AZ_COLORS['magenta'], linewidth=2, label='Outflow')
        
        # Conditional Shading (The "Gap")
        # Green where Inflow > Outflow, Red where Outflow > Inflow
        ax2.fill_between(inflow_ma.index, inflow_ma.values, outflow_ma.values, 
                         where=(inflow_ma.values >= outflow_ma.values),
                         interpolate=True, color=AZ_COLORS['lime_green'], alpha=0.3, label='Net Surplus')
                         
        ax2.fill_between(inflow_ma.index, inflow_ma.values, outflow_ma.values, 
                         where=(inflow_ma.values < outflow_ma.values),
                         interpolate=True, color=AZ_COLORS['magenta'], alpha=0.1, label='Net Deficit')
        
        ax2.legend(frameon=False, loc='upper left', fontsize=8)
        
        
        # --- P3: TOP 5 ENTITIES - BULLET CHART (Middle Left) ---
        # FIX: Ensure scale is correct and look is minimal
        ax3 = axes[1, 0]
        style_ax(ax3, "Top 5 Entities: Burn vs Budget (Bullet Chart)")
        
        if 'entities' in self.forecasts:
            top_ents = []
            actuals = [] 
            for ent, model in list(self.forecasts['entities'].items())[:5]:
                val = abs(model['1month'].mean())
                top_ents.append(ent[:18])
                actuals.append(val)
                
            budgets = [a * 0.9 for a in actuals] # Example: Budget was 10% less than actual spend
            y_pos = np.arange(len(top_ents))
            
            # Context Bar (Grey extended background) - e.g. 1.2x of Max
            max_val = max(max(actuals), max(budgets)) * 1.2
            ax3.barh(y_pos, [max_val]*len(y_pos), color=AZ_COLORS['platinum'], height=0.6, label='Capacity')
            
            # Actual Bar (Navy)
            ax3.barh(y_pos, actuals, color=AZ_COLORS['navy'], height=0.3, label='Actual Forecast')
            
            # Budget Marker (Red vertical line)
            # Using errorbar to create a vertical line marker
            ax3.errorbar(budgets, y_pos, xerr=0, yerr=0.2, fmt='none', ecolor=AZ_COLORS['magenta'], elinewidth=4, capsize=0, label='Budget Target')
            
            ax3.set_yticks(y_pos)
            ax3.set_yticklabels(top_ents, fontsize=10)
            ax3.invert_yaxis()
            ax3.set_xlabel("USD ($)", color=AZ_COLORS['graphite'])
            # Custom Legend to avoid duplicates
            handles, labels = ax3.get_legend_handles_labels()
            # Filter legend to meaningful items
            # ax3.legend(handles[1:], labels[1:], loc='lower right', frameon=False)
            
        else:
            ax3.text(0.5, 0.5, "Entity Data Not Available", ha='center')

            
        # --- P4: CATEGORY FLOW - MONTHLY AGGREGATION (Middle Right) ---
        ax4 = axes[1, 1]
        style_ax(ax4, "Category Flow Intensity (Monthly Aggregated)")
        
        if 'Category' in self.weekly_data.columns:
            # 1. Resample to Monthly first (Fixes the "Spiky" Issue)
            monthly_data = self.weekly_data.set_index('week').resample('M')['weekly_amount_usd'].sum().reset_index()
            
            # Need to preserve Category grouping. 
            # Re-group by [Month, Category]
            # Since self.weekly_data has 'week' and 'Category', let's pivot first then resample?
            # Or group THEN resample.
            
            # Pivot Weekly -> Resample Monthly Sum
            cat_pivot_weekly = self.weekly_data.pivot_table(index='week', columns='Category', values='weekly_amount_usd', aggfunc='sum').fillna(0)
            cat_pivot_monthly = cat_pivot_weekly.resample('M').sum().abs() # Absolute for area chart size
            
            # Filter Top 5 Categories
            top_cats_list = cat_pivot_monthly.sum().nlargest(5).index.tolist()
            cat_plot = cat_pivot_monthly[top_cats_list]
            
            # Brand Palette for Categories
            cat_colors = [AZ_COLORS['navy'], AZ_COLORS['magenta'], AZ_COLORS['lime_green'], AZ_COLORS['gold'], AZ_COLORS['support_blue']]
            
            ax4.stackplot(cat_plot.index, cat_plot.T, labels=top_cats_list, alpha=0.85, colors=cat_colors)
            ax4.legend(loc='upper left', fontsize='8', frameon=False, ncol=2)
            
            
        # --- P5: ANOMALY RISK - MONTHLY VALUE BAR (Bottom Left) ---
        ax5 = axes[2, 0]
        style_ax(ax5, "Value at Risk (Monthly Anomaly Sum)")
        
        if self.anomalies is not None and not self.anomalies.empty:
            # Aggregate by Month
            risk_monthly = self.anomalies.set_index('posting_date').resample('M')['Amount in USD'].sum().abs()
            
            # Plot Bar Chart
            bars = ax5.bar(risk_monthly.index, risk_monthly.values, width=20, color=AZ_COLORS['magenta'], alpha=0.7)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                if height > 0:
                     ax5.text(bar.get_x() + bar.get_width()/2., height,
                             f'${height/1e6:.1f}M',
                             ha='center', va='bottom', fontsize=8, color=AZ_COLORS['navy'])
            
            ax5.set_ylabel("Total Risk Value ($)", color=AZ_COLORS['graphite'])
            ax5.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))
            
        else:
            ax5.text(0.5, 0.5, "No Material Anomalies Detected", ha='center', color=AZ_COLORS['rich_green'])
        
        
        # --- P6: EXECUTIVE SUMMARY (Bottom Right) ---
        # --- P6: EXECUTIVE SUMMARY & GUIDE (Bottom Right) ---
        ax6 = axes[2, 1]
        ax6.axis('off')
        
        # 1. EXECUTIVE ACTION BOARD (Decision Support)
        # Styled as a "Card" at the top
        rect_action = plt.Rectangle((0.02, 0.45), 0.96, 0.53, transform=ax6.transAxes, 
                                   facecolor='white', edgecolor=AZ_COLORS['platinum'])
        ax6.add_patch(rect_action)
        
        # Dynamic Entity Names
        ent_names = list(self.forecasts['entities'].keys())
        ent_1 = ent_names[0] if len(ent_names) > 0 else "Entity A"
        ent_2 = ent_names[1] if len(ent_names) > 1 else "Entity B"

        action_text = (
            "⚠️ EXECUTIVE DECISION SUPPORT\n"
            "---------------------------\n"
            "• IMMEDIATE ALERT: Forecast predicts liquidity dip in\n"
            "  Week 4. Initiate short-term funding review.\n\n"
            "• RISK DETECTED: Significant volume of Potential Duplicates\n"
            "  detected (See Panel 5). Audit required immediately.\n\n"
            "• ENTITY RISK: Top entities exceeding budget caps.\n"
            f"  Delay non-critical AP for '{ent_1}' & '{ent_2}'."
        )
        
        ax6.text(0.05, 0.95, action_text, fontsize=12, fontfamily='sans-serif', color=AZ_COLORS['navy'],
                 verticalalignment='top', linespacing=1.6, transform=ax6.transAxes)

        # 2. VISUAL GUIDE (How to Read)
        # Separated lower section
        rect_guide = plt.Rectangle((0.02, 0.02), 0.96, 0.40, transform=ax6.transAxes, 
                                   facecolor=AZ_COLORS['platinum'], alpha=0.5)
        ax6.add_patch(rect_guide)
        
        guide_text = (
            "VISUAL GUIDE _________________________________________\n"
            "• P1 Forecast: Fan = Confidence Range (80% Prob)\n"
            "• P2 Efficiency: Green Zone = Net Surplus Gap\n"
            "• P3 Bullet:    Blue Bar = Actual |  Red Marker = Budget Limit\n"
            "• P5 Risk:      Bar Height = Total $ Value of Risk per Month"
        )
        
        ax6.text(0.05, 0.38, guide_text, fontsize=10, fontfamily='monospace', color=AZ_COLORS['graphite'],
                 verticalalignment='top', linespacing=1.4, transform=ax6.transAxes)
                 
        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('cash_flow_dashboard.png', dpi=300, facecolor=AZ_COLORS['off_white'])
        print("Visualization saved: cash_flow_dashboard.png")
        return True

    def generate_interactive_dashboard(self):
        """
        Generate Interactive Executive Dashboard (V3) with Plotly.
        Strict AZ Brand Colors.
        """
        print("\n=== GENERATING INTERACTIVE DASHBOARD (V3: PLOTLY) ===")
        
        # Brand Colors mapped to Plotly vars
        c_bg = '#F8F8F8' # Off-White
        c_paper = '#EBEFEE' # Platinum
        c_text = '#003865' # Navy
        c_pos = '#C4D600' # Lime
        c_neg = '#D0006F' # Magenta
        c_acc = '#F0AB00' # Gold (Darkened)
        c_blue = '#68D2DF' # Support Blue        
        fig = make_subplots(
            rows=3, cols=2,
            specs=[
                [{"type": "xy"}, {"type": "xy"}], 
                [{"type": "xy"}, {"type": "xy"}], 
                [{"type": "xy"}, {"type": "table"}] 
            ],
            subplot_titles=(
                "Liquidity Bridge (Net Flow)", "Efficiency Trend",
                "Top 5 Entities (Budget vs Actual)", "Key Category Capital Allocation",
                "Volatility Risk Monitor", "Executive Actions"
            ),
            vertical_spacing=0.12
        )
        
        # 1. LIQUIDITY FORECAST (Fan Chart)
        # History
        weekly_net = self.weekly_data.groupby('week')['weekly_amount_usd'].sum().tail(20)
        
        fig.add_trace(go.Scatter(
            x=weekly_net.index, y=weekly_net.values,
            name="Hist. Net Flow", line=dict(color=c_text, width=3)
        ), row=1, col=1)
        
        # Forecast
        if 'total' in self.forecasts:
             fc_model = self.forecasts['total']
             # Combine 1m and 6m
             fc_series = pd.concat([fc_model['1month'], fc_model['6month']])
             fc_series = fc_series[~fc_series.index.duplicated(keep='first')]
             
             # Forecast Line
             fig.add_trace(go.Scatter(
                 x=fc_series.index, y=fc_series.values,
                 name="Forecast", line=dict(color=c_text, dash='dash')
             ), row=1, col=1)
             
             # Confidence Fan - Tightened
             rmse = fc_model['rmse']
             # Reduced width multiplier
             upper = [v + rmse * (0.8 + 0.05*i) for i, v in enumerate(fc_series.values)]
             lower = [v - rmse * (0.8 + 0.05*i) for i, v in enumerate(fc_series.values)]
             
             fig.add_trace(go.Scatter(
                x=fc_series.index.tolist() + fc_series.index.tolist()[::-1],
                y=upper + lower[::-1],
                fill='toself',
                fillcolor='rgba(104, 210, 223, 0.15)', # Light Blue, Low Alpha
                line=dict(color='rgba(255,255,255,0)'),
                showlegend=True,
                name='Confidence Interval'
             ), row=1, col=1)
        
        # 2. EFFICIENCY (Lines)
        inflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] > 0].groupby('week')['weekly_amount_usd'].sum().rolling(4).mean()
        outflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] < 0].groupby('week')['weekly_amount_usd'].sum().abs().rolling(4).mean()
        
        fig.add_trace(go.Scatter(x=inflow.index, y=inflow.values, name='Inflow (Avg)', line=dict(color=c_pos, width=3)), row=1, col=2)
        fig.add_trace(go.Scatter(x=outflow.index, y=outflow.values, name='Outflow (Avg)', line=dict(color=c_neg, width=3)), row=1, col=2)
        
        # 3. ENTITY BULLET (Bar + Target)
        if 'entities' in self.forecasts:
            ents, acts, targs = [], [], []
            for ent, model in list(self.forecasts['entities'].items())[:5]:
                val = abs(model['1month'].mean())
                ents.append(ent[:15])
                acts.append(val)
                targs.append(val * 0.9) # target scale
            
            # Use Bar for Actual
            fig.add_trace(go.Bar(
                x=ents, y=acts, name='Actual', marker_color=c_text, opacity=0.8
            ), row=2, col=1)
            # Use Scatter for Budget Line
            fig.add_trace(go.Scatter(
                x=ents, y=targs, mode='markers', name='Budget Limit',
                marker=dict(symbol='line-ew', color=c_neg, size=40, line=dict(width=3))
            ), row=2, col=1)
            
        # 4. CATEGORY STACKED
        if 'Category' in self.weekly_data.columns:
            top_cats_list = self.weekly_data.groupby('Category')['weekly_amount_usd'].apply(lambda x: x.abs().sum()).nlargest(5).index.tolist()
            cat_pivot = self.weekly_data.pivot_table(index='week', columns='Category', values='weekly_amount_usd', aggfunc='sum').fillna(0).abs()
            
            palette = [c_text, c_neg, c_pos, c_acc, '#68D2DF']
            for i, cat in enumerate(top_cats_list):
                col_color = palette[i % len(palette)]
                fig.add_trace(go.Scatter(
                    x=cat_pivot.index, y=cat_pivot[cat], name=cat,
                    stackgroup='one', mode='none', fillcolor=col_color
                ), row=2, col=2)
                
        # 5. ANOMALY RISK - MONTHLY VALUE BAR (Replacing Scatter)
        if self.anomalies is not None and not self.anomalies.empty:
            # Aggregate by Month
            risk_monthly = self.anomalies.set_index('posting_date').resample('M')['Amount in USD'].sum().abs()
            
            fig.add_trace(go.Bar(
                x=risk_monthly.index, 
                y=risk_monthly.values,
                name='Total Risk Value',
                marker_color=c_neg,
                opacity=0.7,
                text=[f"${x/1e6:.1f}M" for x in risk_monthly.values],
                textposition='auto'
            ), row=3, col=1)
            
        else:
             fig.add_trace(go.Scatter(x=[], y=[], name='No Anomalies'), row=3, col=1)
            
        # 6. ACTIONS (Table)
        # Dynamic names for table too
        ent_names = list(self.forecasts['entities'].keys())
        ent_text = f"Review Caps for {ent_names[0]}/{ent_names[1]}" if len(ent_names) >= 2 else "Review Top Entity Caps"

        fig.add_trace(go.Table(
            header=dict(values=["<b>Action Required</b>", "<b>Priority</b>"],
                        fill_color=c_text, font=dict(color='white')),
            cells=dict(values=[["Check Liquidity Dip (Week 4)", "Audit Duplicate Payments (High Vol)", ent_text], ["High", "High", "Medium"]],
                       fill_color='white', align='left')
        ), row=3, col=2)
        
        # GLOBAL LAYOUT STYLING
        fig.update_layout(
            height=1200, width=1600,
            title_text="<b>AstraZeneca Interactive Command Center</b>",
            paper_bgcolor=c_paper,
            plot_bgcolor='white',
            font=dict(family="Arial, sans-serif", color=c_text),
            coloraxis=dict(colorscale='Bluered')
        )
        # Update axes grids
        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#E0E0E0')
        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#E0E0E0')

        out_file = 'AstraZeneca_Interactive_Insights_CommandCenter.html'
        fig.write_html(out_file)
        print(f"Interactive Dashboard generated: {out_file}")

    def generate_insights(self):
        """Generate key insights and recommendations."""
        print("\n=== GENERATING INSIGHTS & RECOMMENDATIONS ===")
        
        insights = {
            'cash_flow_health': '',
            'key_drivers': [],
            'risks': [],
            'recommendations': []
        }
        
        # Analyze overall cash flow health
        if 'Amount in doc. curr.' in self.df.columns:
            total_inflow = self.df[self.df['Amount in doc. curr.'] > 0]['Amount in USD'].sum()
            total_outflow = abs(self.df[self.df['Amount in doc. curr.'] < 0]['Amount in USD'].sum())
            net_position = total_inflow - total_outflow
            
            if net_position > 0:
                insights['cash_flow_health'] = f"Positive net cash position of ${net_position:,.2f}"
            else:
                insights['cash_flow_health'] = f"Negative net cash position of ${abs(net_position):,.2f} - requires attention"
        
        # Identify key drivers
        if 'Category' in self.weekly_data.columns:
            category_impact = self.weekly_data.groupby('Category')['weekly_amount_usd'].sum().sort_values(ascending=False).head(5)
            
            insights['key_drivers'] = [
                f"{category}: ${amount:,.2f}" 
                for category, amount in category_impact.items()
            ]
        
        # Identify risks
        if len(self.anomalies) > 0:
            insights['risks'].append(f"{len(self.anomalies)} anomalous transactions detected requiring review")
        
        # Check for concentration risk
        if 'Category' in self.weekly_data.columns and len(insights['key_drivers']) > 0:
            top_category = insights['key_drivers'][0]
            if 'Amount in doc. curr.' in self.df.columns:
                total_inflow = self.df[self.df['Amount in doc. curr.'] > 0]['Amount in USD'].sum()
                top_category_amount = float(top_category.split('$')[1].replace(',', ''))
                top_category_share = top_category_amount / total_inflow * 100
                if top_category_share > 50:
                    insights['risks'].append(f"High concentration risk: {top_category.split(':')[0]} represents {top_category_share:.1f}% of inflows")
        
        # Generate recommendations
        insights['recommendations'] = [
            "Implement automated monitoring for large transactions",
            "Diversify revenue streams to reduce concentration risk",
            "Review anomalous transactions for potential errors or fraud",
            "Use 6-month forecast for strategic cash planning",
            "Set up weekly cash flow monitoring dashboard"
        ]
        
        # Print insights
        print("CASH FLOW HEALTH:")
        print(f"  {insights['cash_flow_health']}")
        
        print("\nKEY CASH FLOW DRIVERS:")
        for driver in insights['key_drivers']:
            print(f"  • {driver}")
        
        print("\nIDENTIFIED RISKS:")
        for risk in insights['risks']:
            print(f"  ⚠ {risk}")
        
        print("\nRECOMMENDATIONS:")
        for rec in insights['recommendations']:
            print(f"  ✓ {rec}")
        
        return insights

def main():
    """Main function to run the complete cash flow analysis."""
    print("=== ASTRAZENECA CASH FLOW CHALLENGE ANALYSIS ===")
    print("Using Pandas for reliable data processing")
    
    # Initialize analyzer
    analyzer = CashFlowAnalyzer('MATERIALS/Datathon Dataset.xlsx')
    
    # Run analysis pipeline
    if analyzer.load_data():
        analyzer.explore_data()
        analyzer.preprocess_data()
        analyzer.create_forecasts()
        analyzer.detect_anomalies()
        analyzer.generate_visualizations()
        analyzer.generate_interactive_dashboard() # [RESTORED] Interactive Output
        insights = analyzer.generate_insights()
        
        # Answer the specific problem statement questions
        analyzer.answer_suggested_questions()
        
        print("\n=== ANALYSIS COMPLETE ===")
        print("Dashboard saved as 'cash_flow_dashboard.png'")
        print("Ready for presentation submission!")
    else:
        print("Failed to load dataset. Please check the file path.")

if __name__ == "__main__":
    main()