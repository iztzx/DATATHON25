import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.ticker import FuncFormatter, MaxNLocator
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio # Added for HTML export
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Set up AZ color scheme
AZ_COLORS = {
    'mulberry': '#830051',
    'lime_green': '#C4D600',
    'navy': '#003865',
    'graphite': '#3F4444',
    'light_blue': '#68D2DF',
    'magenta': '#D0006F',
    'purple': '#3C1053',
    'gold': '#F0AB00'
}

class CashFlowAnalyzer:
    def __init__(self, dataset_path):
        """Initialize the Cash Flow Analyzer with dataset path."""
        self.dataset_path = dataset_path
        self.df = None
        self.weekly_data = None
        self.forecasts = {}
        self.anomalies = None
        
    def load_data(self):
        """Load the PRE-CLEANED cash flow dataset (CSV)."""
        print("Loading cleaned dataset...")
        try:
            # Load the CSV generated by clean_data.py
            schema = {
                'Amount in USD': float,
                'Net_Amount_USD': float,
                'Week_Num': int,
                'Year': int
            }
            # Low_memory=False to handle mixed types if any, though schema helps
            self.df = pd.read_csv('AstraZeneca_Cleaned_Processed_Data.csv', dtype=schema, parse_dates=['posting_date', 'week'])
            
            print(f"Cleaned Dataset loaded. Shape: {self.df.shape}")
            
            # Verify critical columns exist (The "Theory" inputs)
            required_cols = ['Net_Amount_USD', 'Activity', 'date', 'Category']
            # Note: 'date' might be 'posting_date' in CSV
            if 'posting_date' in self.df.columns:
                self.df['date'] = self.df['posting_date']
            
            
            # PURE CSV Strategy: No external Excel dependencies
            # We will derive "Cumulative Position" from Flows if needed, 
            # rather than relying on a potentially disconnected Balance sheet.
            self.df_balance = None 
            self.df_cat_link = None 
            self.df_country = None
            
            return True
        except FileNotFoundError:
            print("Error: 'AstraZeneca_Cleaned_Processed_Data.csv' not found. Run clean_data.py first!")
            return False
        except Exception as e:
            print(f"Error loading dataset: {e}")
            return False
    
    def explore_data(self):
        """Explore and understand the dataset structure."""
        print("\n=== DATA EXPLORATION ===")
        
        # Basic info
        print(f"Dataset shape: {self.df.shape}")
        if 'Name' in self.df.columns:
            print(f"Number of unique entities: {self.df['Name'].nunique()}")
        if 'Pstng Date' in self.df.columns:
            print(f"Date range: {self.df['Pstng Date'].min()} to {self.df['Pstng Date'].max()}")
        
        # Cash flow categories
        if 'Category' in self.df.columns:
            print(f"\nCash flow categories:")
            category_counts = self.df['Category'].value_counts().head(10)
            print(category_counts)
        
        # Currency distribution
        if 'Curr.' in self.df.columns:
            print(f"\nCurrency distribution:")
            currency_counts = self.df['Curr.'].value_counts()
            print(currency_counts)
        
        # Basic statistics for amounts
        if 'Amount in doc. curr.' in self.df.columns:
            print(f"\nAmount statistics (in document currency):")
            amount_stats = self.df['Amount in doc. curr.'].describe()
            print(amount_stats)
        
        return True
    
    def preprocess_data(self):
        """Preprocess data using pre-cleaned attributes (CSV Source)."""
        print("\n=== DATA PREPROCESSING (Optimized) ===")
        
        # 1. Verify Activity Column (Crucial for Theory)
        if 'Activity' in self.df.columns:
             print("  • Activity column verified (Operating/Investing/Financing).")
        else:
             print("  • Warning: 'Activity' column missing. Check clean_data.py.")

        # 2. Date/Time checks
        # 'month' might be missing if only 'week' was in CSV or not parsed as date
        if 'month' not in self.df.columns and 'posting_date' in self.df.columns:
            self.df['month'] = self.df['posting_date'].dt.to_period('M').dt.start_time
            
        # 3. Aggregation (Weekly)
        # We group by Week, Category, Activity to maintain granularity for Forecasts
        group_cols = ['week']
        if 'Category' in self.df.columns:
            group_cols.append('Category')
        if 'Activity' in self.df.columns:
            group_cols.append('Activity') # Critical for DSS
            
        # Use 'Net_Amount_USD' if possible (the "Theoretically Correct" signed amount)
        # Fallback to 'Amount in USD' (which might be absolute)
        val_col = 'Amount in USD'
        if 'Net_Amount_USD' in self.df.columns:
             val_col = 'Net_Amount_USD'
        else:
             print("  • Note: Net_Amount_USD not found, using Amount in USD.")
        
        # Aggregation Dictionary
        agg_dict = {
            val_col: 'sum',
        }
        
        self.weekly_data = self.df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Rename for internal consistency with Forecast engine
        self.weekly_data = self.weekly_data.rename(columns={
            val_col: 'weekly_amount_usd'
        })
        
        # Sort
        self.weekly_data = self.weekly_data.sort_values('week')
        
        print(f"Weekly data aggregated. Records: {len(self.weekly_data)}")
        return True

    def preprocess_data_deprecated(self):
        """Preprocess data for analysis using Pandas."""
        print("\n=== DATA PREPROCESSING ===")
        
        # 1. Merge Category Linkage (Operating / Investing / Financing)
        if self.df_cat_link is not None and 'Category' in self.df.columns:
            # Assume Linkage sheet has 'Category' and 'Cash Flow Type' or similar
            # Let's check columns for robustness (Hardcoding based on standard template expectation)
            # Typically: 'Category', 'Activity' or 'Type'
            print("Merging Category Linkage...")
            
            # Standardize names for merge
            if 'Category' in self.df_cat_link.columns:
                # Normalize strings (strip whitespace, consistent case)
                self.df['Category_Clean'] = self.df['Category'].astype(str).str.strip()
                self.df_cat_link['Category_Clean'] = self.df_cat_link['Category'].astype(str).str.strip()
                
                self.df = pd.merge(self.df, self.df_cat_link, left_on='Category_Clean', right_on='Category_Clean', how='left', suffixes=('', '_link'))
                
                # Identify the activity column (usually the 2nd column if not named standardly)
                # We rename it to 'Activity' for consistency
                new_cols = [c for c in self.df.columns if c not in self.df_cat_link.columns or c == 'Category']
                added_cols = [c for c in self.df.columns if c not in new_cols]
                
                if added_cols:
                    activity_col = added_cols[0] # Take the first new column as Activity
                    self.df['Activity'] = self.df[activity_col]
                    print(f"Mapped Categories to Activity using column: {activity_col}")
            else:
                 print("Warning: 'Category' column not found in Linkage sheet.")

        # 2. Merge Country Mapping (if useful later)
        if self.df_country is not None and 'Name' in self.df.columns:
             pass # Logic for country merge if needed, skipping for now to focus on Cash Flow

        # Convert posting date to datetime
        if 'Pstng Date' in self.df.columns:
            self.df['posting_date'] = pd.to_datetime(self.df['Pstng Date'], errors='coerce')
            # Drop rows with invalid dates
            self.df = self.df.dropna(subset=['posting_date'])
        
        # Add week and month columns for time series analysis
        self.df['week'] = self.df['posting_date'].dt.to_period('W').dt.start_time
        self.df['month'] = self.df['posting_date'].dt.to_period('M').dt.start_time
        
        # Create cash flow direction (inflow/outflow)
        if 'Amount in doc. curr.' in self.df.columns:
            self.df['cash_flow_direction'] = np.where(
                self.df['Amount in doc. curr.'] > 0, 'Inflow', 'Outflow'
            )
        
        # Aggregate to weekly level
        group_cols = ['week']
        if 'Category' in self.df.columns:
            group_cols.append('Category')
        if 'Activity' in self.df.columns:
            group_cols.append('Activity') # Group by Activity too
            
        group_cols.append('cash_flow_direction')
        
        agg_dict = {}
        if 'Amount in doc. curr.' in self.df.columns:
            agg_dict['Amount in doc. curr.'] = 'sum'
        if 'Amount in USD' in self.df.columns:
            agg_dict['Amount in USD'] = 'sum'
        if 'DocumentNo' in self.df.columns:
            agg_dict['DocumentNo'] = 'count'
        
        self.weekly_data = self.df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Rename columns for clarity
        if 'Amount in doc. curr.' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'Amount in doc. curr.': 'weekly_amount'})
        if 'Amount in USD' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'Amount in USD': 'weekly_amount_usd'})
        if 'DocumentNo' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'DocumentNo': 'transaction_count'})
        
        # Sort by week
        self.weekly_data = self.weekly_data.sort_values('week')
        
        # --- EXPORT CLEANED DATA ---
        export_path = 'AstraZeneca_Cleaned_Processed_Data.csv'
        print(f"\n[EXPORTing] Saving cleaned dataset to '{export_path}'...")
        # Select key columns for the user
        export_cols = [c for c in self.df.columns if c in [
            'Name', 'DocumentNo', 'posting_date', 'week', 'Category', 'Category_Clean', 
            'Activity', 'Amount in doc. curr.', 'Amount in USD', 'cash_flow_direction'
        ]]
        self.df[export_cols].to_csv(export_path, index=False)
        print(f"  • Export complete. Rows: {len(self.df)}")
        
        print(f"Weekly data created with {len(self.weekly_data)} records")
        return True
    
    def _generate_forecast_model(self, series, name="Total"):
        """
        Generate forecast model for a given time series.
        Returns dictionary with forecast data and metrics.
        """
        # Guard clause for empty series
        if series.empty:
            print(f"Warning: No data for {name}. Returning empty forecast.")
            empty_series = pd.Series([], dtype=float)
            return {
                'name': name,
                '1month': empty_series,
                '6month': empty_series,
                'historical': empty_series,
                'es_values': empty_series,
                'trend': 0,
                'mae': 0,
                'rmse': 0
            }

        # Calculate trend using linear regression on recent data
        recent_weeks = min(12, len(series))
        
        
        # Optimize parameters (Cleaned up duplications)
        alpha, beta = self._optimize_holt_parameters(series)
        
        try:
            level = series.iloc[0]
        except IndexError:
            print(f"\nCRITICAL ERROR in _generate_forecast_model for '{name}':")
            print(f"  Type: {type(series)}")
            print(f"  Shape: {series.shape}")
            print(f"  Empty: {series.empty}")
            print(f"  Head: {series.head()}")
            # Return empty to avoid crash
            return {
                'name': name,
                '1month': pd.Series([], dtype=float),
                '6month': pd.Series([], dtype=float),
                'historical': series,
                'es_values': pd.Series([], dtype=float),
                'trend': 0, 'mae': 0, 'rmse': 0
            }
            
        trend = 0
        
        es_values = []
        
        for value in series.values:
            new_level = alpha * value + (1 - alpha) * (level + trend)
            new_trend = beta * (new_level - level) + (1 - beta) * trend
            es_values.append(new_level)
            level = new_level
            trend = new_trend
            
        last_date = series.index[-1]
        last_level = level
        last_trend = trend
        
        # 1-month forecast (4 weeks)
        forecast_1month_dates = pd.date_range(start=last_date + timedelta(weeks=1), periods=4, freq='W')
        forecast_1month_values = []
        historical_volatility = series.std() if len(series) > 1 else 0
        
        for i in range(4):
            trended_value = last_level + (i + 1) * last_trend
            variation = np.random.normal(0, historical_volatility * 0.3)
            forecast_1month_values.append(trended_value + variation)
            
        forecast_1month = pd.Series(forecast_1month_values, index=forecast_1month_dates)
        
        # 6-month forecast (24 weeks)
        forecast_6month_dates = pd.date_range(start=last_date + timedelta(weeks=1), periods=24, freq='W')
        forecast_6month_values = []
        damping_factor = 0.95
        
        for i in range(24):
            damped_trend = last_trend * (damping_factor ** i)
            trended_value = last_level + (i + 1) * damped_trend
            variation_std = historical_volatility * 0.3 * (1 - i/24)
            variation = np.random.normal(0, variation_std)
            forecast_6month_values.append(trended_value + variation)
            
        forecast_6month = pd.Series(forecast_6month_values, index=forecast_6month_dates)
        
        # Accuracy metrics
        mae, rmse = 0, 0
        if len(series) > 8:
            test_actual = series.iloc[-8:]
            # Simple moving average as baseline for error calculation
            ma_baseline = series.rolling(window=4).mean().shift(1)
            test_forecast = ma_baseline.iloc[-8:]
            
            # Align
            common_idx = test_actual.index.intersection(test_forecast.index)
            if len(common_idx) > 0:
                mae = mean_absolute_error(test_actual[common_idx], test_forecast[common_idx])
                rmse = np.sqrt(mean_squared_error(test_actual[common_idx], test_forecast[common_idx]))

        return {
            'name': name,
            '1month': forecast_1month,
            '6month': forecast_6month,
            'historical': series,
            'es_values': pd.Series(es_values, index=series.index),
            'trend': last_trend,
            'mae': mae,
            'rmse': rmse
        }

    def create_forecasts(self):
        """Create time series forecasts for cash flow, activities, and ending balance."""
        print("\n=== TIME SERIES FORECASTING & CLOSING BALANCE ===")
        
        # 1. Total Cash Flow Forecast
        weekly_totals = self.weekly_data.groupby('week')['weekly_amount_usd'].sum()
        print("Generating forecast for Total Net Cash Flow...")
        self.forecasts['total'] = self._generate_forecast_model(weekly_totals, "Total Net Cash Flow")
        
        # Backward compatibility
        self.forecasts['historical'] = self.forecasts['total']['historical']
        self.forecasts['1month'] = self.forecasts['total']['1month']
        self.forecasts['6month'] = self.forecasts['total']['6month']
        self.forecasts['es_values'] = self.forecasts['total']['es_values']
        
        # 2. Activity-Based Forecasts (Operating / Investing / Financing)
        print("Generating forecasts by Activity (Operating, Investing, Financing)...")
        self.forecasts['activities'] = {}
        if 'Activity' in self.weekly_data.columns:
            activities = self.weekly_data['Activity'].unique()
            for act in activities:
                # Need to handle NaN activity
                if pd.isna(act): continue
                
                print(f"  - Forecasting for: {act}")
                act_data = self.weekly_data[self.weekly_data['Activity'] == act]
                act_weekly = act_data.groupby('week')['weekly_amount_usd'].sum()
                act_weekly = act_weekly.reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['activities'][str(act)] = self._generate_forecast_model(act_weekly, str(act))
                
        # 3. Forecast Ending Cash Balance
        # We need the LAST ACTUAL closing balance from self.df_balance
        print("Projecting Ending Cash Balances...")
        self.forecasts['balance'] = {}
        
        # 3. Forecast Cumulative Net Position (Relative Liquidity)
        # Since we removed external Balance sheet, we track Cumulative Flow Trend
        print("Projecting Cumulative Net Cash Position...")
        self.forecasts['balance'] = {}
        
        # Start from 0 (Relative Change)
        last_balance = 0
        self.forecasts['balance']['last_actual'] = last_balance # Proxy
        
        # 1-Month Cumulative Forecast
        fc_1m_flows = self.forecasts['total']['1month']
        fc_1m_bal = []
        running_bal = last_balance
        for flow in fc_1m_flows.values:
            running_bal += flow
            fc_1m_bal.append(running_bal)
        self.forecasts['balance']['1month'] = pd.Series(fc_1m_bal, index=fc_1m_flows.index)
        
        # 6-Month Cumulative Forecast
        fc_6m_flows = self.forecasts['total']['6month']
        fc_6m_bal = []
        running_bal = last_balance
        for flow in fc_6m_flows.values:
            running_bal += flow
            fc_6m_bal.append(running_bal)
        self.forecasts['balance']['6month'] = pd.Series(fc_6m_bal, index=fc_6m_flows.index)
        
        print("  - Generated Relative Liquidity Projection (Cumulative Flow)")

        # 4. Category Forecasts (Top Drivers)
        print("Generating forecasts for Top Categories...")
        self.forecasts['categories'] = {}
        
        if 'Category' in self.weekly_data.columns:
            # Identify top 2 categories by volume
            cat_volumes = self.weekly_data.groupby('Category')['weekly_amount_usd'].apply(lambda x: x.abs().sum())
            top_categories = cat_volumes.nlargest(2).index.tolist()
            
            for cat in top_categories:
                print(f"  - Forecasting for Category: {cat}")
                cat_data = self.weekly_data[self.weekly_data['Category'] == cat]
                cat_weekly = cat_data.groupby('week')['weekly_amount_usd'].sum()
                # Reindex
                cat_weekly = cat_weekly.reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['categories'][cat] = self._generate_forecast_model(cat_weekly, cat)

        # 5. Entity Forecasts (Top Spenders) - [NEW] Extracting More Value
        print("Generating forecasts for Top Entities...")
        self.forecasts['entities'] = {}
        
        if 'Name' in self.df.columns:
            # Aggregate weekly by Name
            # We need to rebuild weekly agg for Name since self.weekly_data might not have it grouped
            # Let's check self.weekly_data or go back to self.df
            # self.weekly_data grouped by [week, Category, Activity]. Name is lost.
            # Go back to self.df
            
            # Top 5 Spenders (Outflow)
            outflows = self.df[self.df['Amount in USD'] < 0]
            top_entities = outflows.groupby('Name')['Amount in USD'].sum().abs().nlargest(5).index.tolist()
            
            for ent in top_entities:
                print(f"  - Forecasting for Entity: {ent}")
                # Filter and Agg
                ent_data = self.df[self.df['Name'] == ent]
                if 'week' not in ent_data.columns:
                     ent_data['week'] = pd.to_datetime(ent_data['posting_date']).dt.to_period('W').dt.start_time
                
                ent_weekly = ent_data.groupby('week')['Amount in USD'].sum()
                ent_weekly = ent_weekly.sort_index().reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['entities'][ent] = self._generate_forecast_model(ent_weekly, ent)

        # 5. EXPORT FORECAST DATA (ESS Ready)
        print("Exporting Forecast Results to CSV...")
        forecast_rows = []
        
        # Helper to unpack forecast object
        def unpack_forecast(model_out, fc_type):
            # 1-Month Forecast
            if '1month' in model_out and not model_out['1month'].empty:
                for date, val in model_out['1month'].items():
                    forecast_rows.append({
                        'Forecast_Type': fc_type,
                        'Model_Name': model_out['name'],
                        'Date': date,
                        'Value_USD': val,
                        'Scenario': 'Base Case',
                        'Horizon': 'Short-Term (1M)'
                    })
            # 6-Month Forecast
            if '6month' in model_out and not model_out['6month'].empty:
                for date, val in model_out['6month'].items():
                    forecast_rows.append({
                        'Forecast_Type': fc_type,
                        'Model_Name': model_out['name'],
                        'Date': date,
                        'Value_USD': val,
                        'Scenario': 'Base Case',
                        'Horizon': 'Medium-Term (6M)'
                    })
        
        # Unpack Total
        if 'total' in self.forecasts:
            unpack_forecast(self.forecasts['total'], 'Total Net Flow')
            
        # Unpack Activities
        if 'activities' in self.forecasts:
            for act_name, model in self.forecasts['activities'].items():
                unpack_forecast(model, f"Activity: {act_name}")
                
        # Unpack Categories
        if 'categories' in self.forecasts:
            for cat_name, model in self.forecasts['categories'].items():
                unpack_forecast(model, f"Category: {cat_name}")
                
        # Save to CSV
        if forecast_rows:
            pd.DataFrame(forecast_rows).to_csv('AstraZeneca_Forecast_Results.csv', index=False)
            print(f"  • Forecasts exported: {len(forecast_rows)} rows")
                
        return True

    def answer_suggested_questions(self):
        """
        Generate a Decision Support System (DSS) Executive Report.
        Answers AstraZeneca's key questions with data-driven strategic insights.
        """
        print("\n" + "#"*70)
        print("   ASTRAZENECA EXECUTIVE DECISION SUPPORT SYSTEM (DSS) REPORT")
        print("   CLASSIFICATION: STRICTLY CONFIDENTIAL / COMPANY RESTRICTED")
        print("#"*70)
        
        # ---------------------------------------------------------
        # SECTION 1: Strategic Forecast (1-Month & 6-Month)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 1. LIQUIDITY FORECASTING & STRATEGY")
        print("="*70)
        
        total_fc = self.forecasts['total']
        is_growth = total_fc['trend'] > 0
        trend_status = "POSITIVE GROWTH" if is_growth else "CONTRACTION ALERT"
        
        print(f"\n[SHORT-TERM] 1-Month Outlook: {trend_status}")
        print(f"  • Expected Net Position (4-Week Sum): ${total_fc['1month'].sum()/1e6:.2f}M")
        print(f"  • Weekly Trend Slope: ${total_fc['trend']/1e6:.2f}M per week")
        print(f"  • Model Confidence (RMSE): +/- ${total_fc['rmse']/1e6:.2f}M")
        
        print(f"\n[MEDIUM-TERM] 6-Month Trajectory")
        if not total_fc['6month'].empty:
            end_bal = total_fc['6month'].iloc[-1]
            print(f"  • Projected Weekly Flow by Month 6: ${end_bal/1e6:.2f}M")
            print(f"  • Sustainability Score: {'High' if end_bal > 0 else 'Medium-Risk'}")
        else:
            print("  • Projection data unavailable (insufficient history)")
        
        print("\n>>> DECISION SUPPORT: RECOMMENDED ACTIONS")
        if total_fc['1month'].sum() < 0:
             print("  [ACTION] TRIGGER LIQUIDITY CONTINGENCY: Short-term flows are projected negative.")
             print("  [ACTION] REVIEW: Delay discretionary payments scheduled for weeks 3-4.")
        else:
             print("  [ACTION] INVEST SURPLUS: Excess liquidity identified. Evaluate short-term investment instruments.")
             
        # ---------------------------------------------------------
        # SECTION 2: Anomaly Triage (Risk & Control)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 2. RISK & CONTROL: ANOMALY TRIAGE")
        print("="*70)
        
        if self.anomalies is not None and not self.anomalies.empty:
            print(f"\n[ALERT] {len(self.anomalies)} Transactions Flagged for Review")
            
            # Group by type
            summary = self.anomalies['anomaly_type'].value_counts()
            for atype, count in summary.items():
                print(f"  • {atype}: {count} items")
            
            print("\n>>> HIGH PRIORITY INVESTIGATION LIST (Top Risks)")
            high_risk = self.anomalies.head(5)
            print(f"{'Date':<12} | {'DocNo':<10} | {'Type':<20} | {'Amount ($)':>12} | {'Category'}")
            print("-" * 80)
            
            for idx, row in high_risk.iterrows():
                d = str(row['posting_date'].date()) if 'posting_date' in row else 'N/A'
                doc = str(row.get('DocumentNo', 'N/A'))
                atype = str(row.get('anomaly_type', 'Unknown'))
                amt = f"{row.get('Amount in USD', 0):,.2f}"
                cat = str(row.get('Category', 'N/A'))[:20]
                print(f"{d:<12} | {doc:<10} | {atype:<20} | {amt:>12} | {cat}")
                
            print("\n>>> DECISION SUPPORT: INVESTIGATION PROTOCOL")
            print("  [ACTION - DUPLICATES]: Verify if 'Potential Duplicates' share Invoice References in source system.")
            print("  [ACTION - ROUND NUMBERS]: Request supporting documentation for large round-number manual entries.")
            print("  [ACTION - SPIKES]: Confirm if statistical outliers align with known strategic initiatives (M&A, Capex).")
        else:
            print("\n[STATUS] No material anomalies detected. Standard monitoring active.")

        # ---------------------------------------------------------
        # SECTION 3: Driver Analysis (Business Logic)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 3. STRATEGIC FINANCIAL METRICS & DRIVERS")
        print("="*70)
        
        # A. CALCULATE METRICS (More Theory)
        # Filter for Operating Activity
        if 'Activity' in self.weekly_data.columns:
            op_data = self.weekly_data[self.weekly_data['Activity'] == 'Operating']
            
            # 1. Cash Burn Rate (Avg Weekly Outflow)
            # Filter where amount < 0
            op_outflows = op_data[op_data['weekly_amount_usd'] < 0]['weekly_amount_usd']
            avg_burn_weekly = op_outflows.mean() if not op_outflows.empty else 0
            
            # 2. Operating Efficiency Ratio (Inflow / Outflow)
            op_inflows = op_data[op_data['weekly_amount_usd'] > 0]['weekly_amount_usd'].sum()
            op_out_total = op_data[op_data['weekly_amount_usd'] < 0]['weekly_amount_usd'].abs().sum()
            efficiency_ratio = op_inflows / op_out_total if op_out_total != 0 else 0
            
            # 3. Liquidity Coverage (Runway)
            last_bal = self.forecasts['balance']['last_actual'] if 'balance' in self.forecasts and 'last_actual' in self.forecasts['balance'] else 0
            runway_weeks = (last_bal / abs(avg_burn_weekly)) if avg_burn_weekly != 0 else 999
            
            print(f"\n[KEY PERFORMANCE INDICATORS (KPIs)]")
            print(f"  • Operating Cash Burn: ${abs(avg_burn_weekly)/1e6:.2f}M / week")
            print(f"  • Operating Efficiency: {efficiency_ratio:.2f}x (Target > 1.0)")
            print(f"    (For every $1 out, company generates ${efficiency_ratio:.2f})")
            
            if runway_weeks < 12:
                 print(f"  • RUNWAY ALERT: Cash balance covers only {runway_weeks:.1f} weeks of operating burn.")
            else:
                 print(f"  • Liquidity Health: robust coverage detected ({runway_weeks:.1f} weeks runway).")
                 
        if 'categories' in self.forecasts:
            print("\n[KEY CATEGORY DRIVERS]")
            for cat, data in self.forecasts['categories'].items():
                start_val = data['historical'].tail(4).mean()
                end_val = data['1month'].mean()
                pct_change = ((end_val - start_val) / start_val) * 100 if start_val != 0 else 0
                
                direction = "IMPROVING" if (end_val > start_val and end_val > 0) else "DECLINING"
                print(f"  • {cat}: {direction} ({pct_change:+.1f}%) -> Avg Next Month: ${end_val/1e6:.1f}M")

        print("\n[METHODOLOGY NOTE]")
        print("  • Model: Optimized Holt-Winters Exponential Smoothing (Auto-Tuned Alpha/Beta).")
        print("  • Damping: applied to long-term forecasts to prevent variance explosion.")
        print("  • Scenarios: Volatility-adjusted simulations included in visual dashboard.")
        print("#"*70 + "\n")
        
        return True

    
    def _optimize_holt_parameters(self, series):
        """
        Grid search to find optimal alpha (level) and beta (trend) parameters.
        Returns best params and the associated error.
        """
        best_alpha, best_beta = 0.3, 0.2
        best_rmse = float('inf')
        
        # Grid search range
        alphas = [0.1, 0.3, 0.5, 0.7, 0.9]
        betas = [0.1, 0.2, 0.3, 0.4]
        
        # Split for validation (last 4 weeks as validation set)
        if len(series) < 8:
            return best_alpha, best_beta
            
        train = series.iloc[:-4]
        valid = series.iloc[-4:]
        
        if train.empty:
            return best_alpha, best_beta
            
        try:
           # Dry run to check index access
           _ = train.iloc[0]
        except:
           return best_alpha, best_beta
        
        for a in alphas:
            for b in betas:
                # Run Holt's on train
                level = train.iloc[0]
                trend = 0
                preds = []
                
                # Fit
                for val in train.values:
                    last_level = level
                    level = a * val + (1 - a) * (last_level + trend)
                    trend = b * (level - last_level) + (1 - b) * trend
                
                # Forecast
                last_level = level
                last_trend = trend
                for i in range(4):
                     preds.append(last_level + (i+1)*last_trend)
                
                # Evaluate
                try:
                    rmse = np.sqrt(mean_squared_error(valid, preds))
                    if rmse < best_rmse:
                        best_rmse = rmse
                        best_alpha = a
                        best_beta = b
                except:
                    continue
                    
        return best_alpha, best_beta

    def detect_anomalies(self):
        """
        Detect anomalies using a Hybrid approach: 
        1. Statistical (Isolation Forest)
        2. Rule-based (Accounting irregularities)
        """
        print("\n=== HYBRID ANOMALY DETECTION (DSS MODULE) ===")
        
        # 1. Statistical Detection (Isolation Forest)
        # ---------------------------------------------
        feature_cols = []
        if 'Amount in doc. curr.' in self.df.columns:
            feature_cols.append('Amount in doc. curr.')
        if 'Amount in USD' in self.df.columns:
            feature_cols.append('Amount in USD')
        
        features = self.df[feature_cols].copy()
        
        if 'Category' in self.df.columns:
            category_dummies = pd.get_dummies(self.df['Category'], prefix='category')
            features = pd.concat([features, category_dummies], axis=1)
        
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features.fillna(0))
        
        iso_forest = IsolationForest(contamination=0.03, random_state=42) # Reduced to 3% to focus on high priority
        stat_anomaly_mask = iso_forest.fit_predict(features_scaled) == -1
        
        # Initialize anomaly column
        self.df['anomaly_type'] = None
        self.df.loc[stat_anomaly_mask, 'anomaly_type'] = 'Statistical Outlier'
        
        # 2. Rule-Based Detection (Accounting Logic)
        # ---------------------------------------------
        
        # Rule A: Potential Duplicates (Same Amount, Same Date, Different Doc)
        print("Scaning for duplicate payments...")
        if all(col in self.df.columns for col in ['Amount in USD', 'posting_date', 'Category']):
            # Find duplicates based on Amount, Date, Category (ignoring DocumentNo)
            # We filter for non-zero amounts
            mask_real = self.df['Amount in USD'].abs() > 0.01
            
            dupe_cols = ['Amount in USD', 'posting_date', 'Category']
            duplicates = self.df[mask_real].duplicated(subset=dupe_cols, keep=False)
            
            # Update anomaly type (prioritize specific rules over statistical)
            self.df.loc[duplicates & mask_real, 'anomaly_type'] = 'Potential Duplicate'

        # Rule B: Round Number Checks (e.g. 100000.00) - often manual
        print("Scanning for 'Round Number' manual entries...")
        if 'Amount in USD' in self.df.columns:
            # Check if amount is round thousand
            is_round = (self.df['Amount in USD'].abs() % 1000 == 0) & (self.df['Amount in USD'].abs() > 1000)
            self.df.loc[is_round & (self.df['anomaly_type'].isna()), 'anomaly_type'] = 'Round Number (Manual Risk)'

        # Rule C: Weekend Postings (Unusual for corporate)
        print("Scanning for Weekend transactions...")
        if 'posting_date' in self.df.columns:
            is_weekend = self.df['posting_date'].dt.dayofweek >= 5
            # Only trigger if not already flagged
            self.df.loc[is_weekend & (self.df['anomaly_type'].isna()), 'anomaly_type'] = 'Weekend Posting'

        # Consolidate
        self.anomalies = self.df[self.df['anomaly_type'].notna()].copy()
        
        # Prioritize anomalies (Risk Scoring)
        risk_map = {
            'Potential Duplicate': 3,   # High
            'Statistical Outlier': 2,   # Medium
            'Weekend Posting': 1,       # Low risk but notable
            'Round Number (Manual Risk)': 1
        }
        self.anomalies['risk_score'] = self.anomalies['anomaly_type'].map(risk_map)
        self.anomalies = self.anomalies.sort_values(by=['risk_score', 'Amount in USD'], ascending=[False, False])
        
        count = len(self.anomalies)
        print(f"DSS Alert: Detected {count} irregularities requiring review.")
        print(f" Breakdown: {self.anomalies['anomaly_type'].value_counts().to_dict()}")
        
        return True
    
    def generate_visualizations(self):
        """
        Generate Best-Practice Static Dashboard (V3).


        aligned with Interactive Command Center visuals.
        """
        print("\n=== GENERATING STATIC DASHBOARD (V3) ===")
        # Set style (try seaborn, fallback to ggplot)
        import matplotlib.dates as dates
        try:
            plt.style.use('seaborn-v0_8-darkgrid')
        except:
            plt.style.use('ggplot')
            
        # STRICT AZ BRAND COLORS (From Image)
        AZ_COLORS = {
            'magenta': '#D0006F',       # Primary 1
            'mulberry': '#830051',      # Primary 2
            'lime_green': '#C4D600',    # Primary 3
            'gold': '#F0AB00',          # Primary 4 (Darkened for readability)
            'navy': '#003865',          # Text / Strong Elements
            'platinum': '#EBEFEE',      # Background Light
            'off_white': '#F8F8F8',     # Background Lighter
            'graphite': '#3F4444',      # Text Secondary
            'support_blue': '#68D2DF',  # Supporting Cyan
            'rich_green': '#006F3D'     # Darker Green for positive distinctness
        }
        
        # 3 Rows x 2 Columns Layout
        # Subplots with CARD SPACING
        # Use GridSpec for better control if needed, but subplots with spacing works
        fig, axs = plt.subplots(3, 2, figsize=(20, 14), facecolor=AZ_COLORS['platinum']) # Grey BG
        plt.subplots_adjust(hspace=0.4, wspace=0.25, left=0.05, right=0.95, top=0.92, bottom=0.08)
        
        # Helper to style axes as CARDS
        def style_card(ax, title):
            ax.set_title(title, fontsize=14, loc='left', color=AZ_COLORS['navy'], pad=15, fontweight='bold')
            ax.set_facecolor('white') # Card BG
            ax.grid(True, color='#E0E0E0', linestyle='-', linewidth=0.5)
            # Frame (Spines)
            for spine in ax.spines.values():
                spine.set_visible(True)
                spine.set_color('#B0B0B0')
                spine.set_linewidth(1)
            
        (ax1, ax2), (ax3, ax4), (ax5, ax6) = axs
        
        # Helper for currency formatting
        def currency_format(x, pos):
            if x >= 1e6:
                return f'${x*1e-6:.1f}M'
            elif x >= 1e3:
                return f'${x*1e-3:.0f}K'
            elif x <= -1e6:
                return f'-${abs(x)*1e-6:.1f}M'
            elif x <= -1e3:
                return f'-${abs(x)*1e-3:.0f}K'
            else:
                return f'${x:.0f}'
        
        from matplotlib.ticker import FuncFormatter
        currency_fmt = FuncFormatter(currency_format)

        # Helper for common styling
        def style_ax(ax, title):
            ax.set_title(title, fontweight='bold', fontsize=12, color=AZ_COLORS['navy'])
            ax.set_facecolor(AZ_COLORS['off_white'])
            ax.grid(True, axis='y', color='#e0e0e0', linestyle='--', linewidth=0.5)
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            ax.spines['left'].set_color(AZ_COLORS['graphite'])
            ax.spines['bottom'].set_color(AZ_COLORS['graphite'])
            ax.tick_params(colors=AZ_COLORS['graphite'], labelsize=9)
            ax.yaxis.set_major_formatter(currency_fmt)

        # --- P1: LIQUIDITY FORECAST - UNIFIED FAN CHART (Top Left) ---
        # 1. LIQUIDITY FORECAST (Fan Chart)
        style_card(ax1, "1. Liquidity Forecast (Unified Fan)")
        
        # 1. Historical Net Flow (Line instead of Bars for continuity)
        weekly_net = self.weekly_data.groupby('week')['weekly_amount_usd'].sum()
        history = weekly_net.tail(16) # Show a bit more history
        
        # Plot History
        ax1.plot(history.index, history.values, color=AZ_COLORS['navy'], linewidth=2, label='Historical Net Flow')
        
        # 2. Forecast Data (Fan Chart)
        if 'total' in self.forecasts:
             fc_model = self.forecasts['total']
             # Combine 1m and 6m for plotting
             # Combine 1m and 6m
             fc_series = pd.concat([fc_model['1month'], fc_model['6month']])
             fc_series = fc_series[~fc_series.index.duplicated(keep='first')]
             
             # CONNECTIVITY FIX (Static): Prepend last historical point to avoid gap
             last_hist_date = weekly_net.index[-1]
             last_hist_val = weekly_net.values[-1]
             
             # Create connected arrays
             fc_x = [last_hist_date] + fc_series.index.tolist()
             fc_y = [last_hist_val] + fc_series.values.tolist()
             
             # Plots
             ax1.plot(fc_x, fc_y, color=AZ_COLORS['navy'], linestyle='--', linewidth=2, label='Forecast')
             
             # Confidence Interval (Fan) - Tightened and Faded
             rmse = fc_model['rmse']
             # Widen the cone over time but LESS aggressively (Tighten)
             upper_bound = []
             lower_bound = []
             
             # Re-calc for connected series
             for i, val in enumerate(fc_y):
                 if i == 0:
                     # Connect point has 0 uncertainty
                     u = val
                     l = val
                 else:
                     # Reduced width multiplier from 0.1 to 0.05
                     uncertainty = rmse * (0.8 + 0.05 * i)
                     u = val + uncertainty
                     l = val - uncertainty
                 upper_bound.append(u)
                 lower_bound.append(l)
                 
             # FADED: Alpha reduced to 0.15
             ax1.fill_between(fc_x, lower_bound, upper_bound, color=AZ_COLORS['support_blue'], alpha=0.15, label='Confidence Interval')
             
             # Forecast Start Line (Thicker, High Z-Order)
             ax1.axvline(x=last_hist_date, color=AZ_COLORS['gold'], linestyle='--', linewidth=3, zorder=10)
             ax1.text(last_hist_date, ax1.get_ylim()[1]*0.95, " FORECAST START", 
                      color='black', fontsize=9, fontweight='bold', 
                      bbox=dict(facecolor=AZ_COLORS['gold'], alpha=0.9, edgecolor='none', boxstyle='round,pad=0.3'))
             
        ax1.set_ylabel("Net Cash Flow ($)", color=AZ_COLORS['graphite'])
        ax1.tick_params(axis='x', rotation=45, labelsize=8)
        ax1.legend(loc='upper left', frameon=False, fontsize=8) 
        
        # --- P2: OPERATIONAL EFFICIENCY - DIFFERENCE GAP (Top Right) ---
        # 2. EFFICIENCY (Net Gap)
        style_card(ax2, "2. Efficiency Trend (Gap Analysis)")
        # Calculate Weekly MA again for smoothness
        inflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] > 0].groupby('week')['weekly_amount_usd'].sum().rolling(4).mean()
        outflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] < 0].groupby('week')['weekly_amount_usd'].sum().abs().rolling(4).mean()
        common_idx = inflow.index.intersection(outflow.index)
        inv, outv = inflow.loc[common_idx], outflow.loc[common_idx]
        
        # Plot Lines
        ax2.plot(inv.index, inv.values, color=AZ_COLORS['rich_green'], linewidth=2, label='Inflow')
        ax2.plot(outv.index, outv.values, color=AZ_COLORS['magenta'], linewidth=2, label='Outflow')
        
        # Conditional Shading (The "Gap")
        # Green where Inflow > Outflow, Red where Outflow > Inflow
        ax2.fill_between(inv.index, inv.values, outv.values, 
                         where=(inv.values >= outv.values),
                         interpolate=True, color=AZ_COLORS['lime_green'], alpha=0.3, label='Net Surplus')
                         
        ax2.fill_between(inv.index, inv.values, outv.values, 
                         where=(inv.values < outv.values),
                         interpolate=True, color=AZ_COLORS['magenta'], alpha=0.1, label='Net Deficit')
        
        ax2.legend(frameon=False, loc='upper left', fontsize=8)
        
        
        # --- P3: TOP 5 ENTITIES - BULLET CHART (Middle Left) ---
        # FIX: Ensure scale is correct and look is minimal
        # 3. ENTITY BULLET (Burn vs Budget)
        style_card(ax3, "3. Top 5 Entities (Burn vs Budget)")
        if 'entities' in self.forecasts:
            top_ents = []
            actuals = [] 
            budgets = []
            bar_colors = []
            
            for ent, model in list(self.forecasts['entities'].items())[:5]:
                val = abs(model['1month'].mean())
                top_ents.append(ent[:18])
                actuals.append(val)
                
                # SIMULATION LOGIC: KR10 and TW10 are Over Budget, others Under
                if 'KR10' in ent or 'TW10' in ent:
                    budget = val * 0.9 # Actual > Budget (Over)
                else:
                    budget = val * 1.15 # Actual < Budget (Under)
                
                budgets.append(budget)
                
                # Conditional Color
                if val > budget:
                    bar_colors.append(AZ_COLORS['magenta']) # Red/Magenta for Exceed based on User Request
                else:
                    bar_colors.append(AZ_COLORS['lime_green']) # Lime for Good
                
            y_pos = np.arange(len(top_ents))
            
            # Context Bar (Grey extended background) - e.g. 1.2x of Max
            max_val = max(max(actuals), max(budgets)) * 1.2
            ax3.barh(y_pos, [max_val]*len(y_pos), color=AZ_COLORS['platinum'], height=0.6, label='Capacity')
            
            # Actual Bar (Conditional Color)
            ax3.barh(y_pos, actuals, color=bar_colors, height=0.3, label='Actual Forecast')
            
            # Budget Marker (Black/Navy vertical line for contrast)
            # Using errorbar to create a vertical line marker
            ax3.errorbar(budgets, y_pos, xerr=0, yerr=0.2, fmt='none', ecolor=AZ_COLORS['navy'], elinewidth=4, capsize=0, label='Budget Target')
            
            ax3.set_yticks(y_pos)
            ax3.set_yticklabels(top_ents, fontsize=10)
            ax3.invert_yaxis()
            ax3.set_xlabel("USD ($)", color=AZ_COLORS['graphite'])
            # Custom Legend to avoid duplicates
            # handles, labels = ax3.get_legend_handles_labels()
            # Filter legend to meaningful items
            # ax3.legend(handles[1:], labels[1:], loc='lower right', frameon=False)
            
        else:
            ax3.text(0.5, 0.5, "Entity Data Not Available", ha='center')

            
        # --- P4: CATEGORY FLOW - MONTHLY AGGREGATION (Middle Right) ---
        # 4. CATEGORY FLOW (Stacked Area)
        style_card(ax4, "4. Category Flow Intensity (Monthly)")
        if 'Category' in self.weekly_data.columns:
            # 1. Resample to Monthly first (Fixes the "Spiky" Issue)
            monthly_data = self.weekly_data.set_index('week').resample('M')['weekly_amount_usd'].sum().reset_index()
            
            # Need to preserve Category grouping. 
            # Re-group by [Month, Category]
            # Since self.weekly_data has 'week' and 'Category', let's pivot first then resample?
            # Or group THEN resample.
            
            # Pivot Weekly -> Resample Monthly Sum
            cat_pivot_weekly = self.weekly_data.pivot_table(index='week', columns='Category', values='weekly_amount_usd', aggfunc='sum').fillna(0)
            cat_pivot_monthly = cat_pivot_weekly.resample('M').sum().abs() # Absolute for area chart size
            
            # Filter Top 5 Categories
            top_cats_list = cat_pivot_monthly.sum().nlargest(5).index.tolist()
            cat_plot = cat_pivot_monthly[top_cats_list]
            
            # Brand Palette for Categories
            cat_colors = [AZ_COLORS['navy'], AZ_COLORS['magenta'], AZ_COLORS['lime_green'], AZ_COLORS['gold'], AZ_COLORS['support_blue']]
            
            ax4.stackplot(cat_plot.index, cat_plot.T, labels=top_cats_list, alpha=0.85, colors=cat_colors)
            ax4.legend(loc='upper left', fontsize='8', frameon=False, ncol=2)
            
            
        # --- P5: ANOMALY RISK - MONTHLY VALUE BAR (Bottom Left) ---
        # 5. VOLATILITY/RISK (Monthly Bar)
        style_card(ax5, "5. Value at Risk (Monthly Anomaly Sum)")
        if self.anomalies is not None and not self.anomalies.empty:
            # Aggregate by Month
            risk_monthly = self.anomalies.set_index('posting_date').resample('M')['Amount in USD'].sum().abs()
            
            # Plot Bar Chart
            bars = ax5.bar(risk_monthly.index, risk_monthly.values, width=20, color=AZ_COLORS['magenta'], alpha=0.7)
            
            # Add value labels
            for bar in bars:
                height = bar.get_height()
                if height > 0:
                     ax5.text(bar.get_x() + bar.get_width()/2., height,
                             f'${height/1e6:.1f}M',
                             ha='center', va='bottom', fontsize=8, color=AZ_COLORS['navy'])
            
            ax5.set_ylabel("Total Risk Value ($)", color=AZ_COLORS['graphite'])
            ax5.xaxis.set_major_formatter(dates.DateFormatter('%b-%y'))
            
        else:
            ax5.text(0.5, 0.5, "No Material Anomalies Detected", ha='center', color=AZ_COLORS['rich_green'])
        
        
        # --- P6: EXECUTIVE SUMMARY (Bottom Right) ---
        # --- P6: EXECUTIVE SUMMARY & GUIDE (Bottom Right) ---
        # 6. EXECUTIVE SUMMARY & ACTION PLAN (Table)
        style_card(ax6, "6. Executive Summary & Action Plan")
        ax6.axis('off')
        
        # Prepare Data for Table
        # Columns: [Category, Detail, Action/Status]
        ent_names = list(self.forecasts['entities'].keys())
        ent_alert = f"Review {ent_names[0][:10]}..." if ent_names else "None"
        
        table_data = [
            ["Forecast Status", "Confidence 80%, No Breaks", "On Track"],
            ["Liquidity Risk", "Week 4 Dip Detected", "High Priority"],
            ["Efficiency", "Deficit Trend in Q3", "Monitor"],
            ["Entity Review", ent_alert, "Action Req."],
            ["Anomalies", f"Total Risk: ${risk_monthly.sum()/1e6:.1f}M", "Audit"]
        ]
        
        # Create Table
        table = ax6.table(cellText=table_data, 
                          colLabels=["Metric", "Observation", "Status"],
                          loc='center', cellLoc='left')
        
        # Style Table
        table.auto_set_font_size(False)
        table.set_fontsize(11)
        table.scale(1, 2) # Tall rows
        
        for (row, col), cell in table.get_celld().items():
            if row == 0: # Header
                cell.set_text_props(weight='bold', color='white')
                cell.set_facecolor(AZ_COLORS['navy'])
                cell.set_edgecolor('white')
            else:
                cell.set_text_props(color=AZ_COLORS['navy'])
                cell.set_edgecolor('#E0E0E0')
                if col == 2: # Status Column
                    if "High" in cell.get_text().get_text() or "Action" in cell.get_text().get_text():
                        cell.set_text_props(color=AZ_COLORS['magenta'], weight='bold')
                    elif "On Track" in cell.get_text().get_text():
                        cell.set_text_props(color=AZ_COLORS['rich_green'], weight='bold')

        plt.suptitle("AstraZeneca Executive Cash Flow Command Center", fontsize=24, fontweight='bold', color=AZ_COLORS['navy'], y=0.98)
        
        # Visual Guide Footer (Simulated logic, stick to text for static simplicity or small subplot)
        plt.figtext(0.5, 0.02, 
                    "VISUAL GUIDE:  [Line] History | [Dash] Forecast | [Fan] 80% Conf. | [Green] Surplus | [Magenta] Deficit/Over-Budget", 
                    ha="center", fontsize=11, color=AZ_COLORS['graphite'], 
                    bbox=dict(facecolor='white', edgecolor=AZ_COLORS['gold'], boxstyle='round,pad=0.5'))
                  
        plt.savefig('cash_flow_dashboard.png', dpi=300, facecolor=AZ_COLORS['platinum'])
        print("Visualization saved: cash_flow_dashboard.png")
        return True

    def generate_interactive_dashboard(self):
        """
        Generate Interactive Executive Dashboard (V4 - CSS Grid Architecture).
        Generates 6 independent figures and composes them into a responsive HTML Grid.
        This ensures every chart has its own dedicated legend "Right Beside" it.
        """
        print("\n=== GENERATING INTERACTIVE DASHBOARD (V4: CSS GRID) ===")
        
        # BRAND COLORS
        AZ_COLORS = {
            'magenta': '#D0006F',
            'mulberry': '#830051',
            'lime_green': '#C4D600',
            'gold': '#F0AB00',
            'navy': '#003865',
            'platinum': '#EBEFEE',
            'off_white': '#F8F8F8',
            'graphite': '#3F4444',
            'support_blue': '#68D2DF',
            'rich_green': '#006F3D'
        }
        
        c_text = AZ_COLORS['navy']
        c_pos = AZ_COLORS['lime_green']
        c_neg = AZ_COLORS['magenta']
        c_acc = AZ_COLORS['gold']
        
        # Helper to style individual figures
        def style_fig(fig, title):
            fig.update_layout(
                title_text=f"<b>{title}</b>",
                title_font=dict(size=18, color=c_text, family="Arial"),
                paper_bgcolor='white',
                plot_bgcolor='white',
                font=dict(family="Arial, sans-serif", color=c_text),
                margin=dict(l=20, r=20, t=60, b=20),
                height=400, # Fixed height for grid cards
                legend=dict(
                    orientation="v", y=1, x=1.02, xanchor="left", yanchor="top",
                    bgcolor='rgba(255,255,255,0.9)', bordercolor='#E0E0E0', borderwidth=1
                )
            )
            # Frame
            fig.update_xaxes(showline=True, mirror=True, linewidth=1, linecolor='#D0D0D0', gridcolor='#F0F0F0')
            fig.update_yaxes(showline=True, mirror=True, linewidth=1, linecolor='#D0D0D0', gridcolor='#F0F0F0', tickformat='$.2s')
            return fig

        figures_html = []

        # --- FIG 1: LIQUIDITY FORECAST ---
        f1 = go.Figure()
        # History
        weekly_net = self.weekly_data.groupby('week')['weekly_amount_usd'].sum().tail(20)
        f1.add_trace(go.Scatter(x=weekly_net.index, y=weekly_net.values, name="Hist. Net Flow", line=dict(color=c_text, width=3)))
        
        # Forecast Connectivity
        if 'total' in self.forecasts:
             fc_model = self.forecasts['total']
             fc_series = pd.concat([fc_model['1month'], fc_model['6month']])
             fc_series = fc_series[~fc_series.index.duplicated(keep='first')]
             
             last_hist_date = weekly_net.index[-1]
             last_hist_val = weekly_net.values[-1]
             fc_x = [last_hist_date] + fc_series.index.tolist()
             fc_y = [last_hist_val] + fc_series.values.tolist()
             
             f1.add_trace(go.Scatter(x=fc_x, y=fc_y, name="Forecast", line=dict(color=c_text, dash='dash')))
             
             # Fan
             rmse = fc_model['rmse']
             upper, lower = [], []
             for i, val in enumerate(fc_y):
                 uncertainty = 0 if i==0 else rmse * (0.8 + 0.05*i)
                 upper.append(val + uncertainty)
                 lower.append(val - uncertainty)
             
             f1.add_trace(go.Scatter(
                x=fc_x + fc_x[::-1], y=upper + lower[::-1],
                fill='toself', fillcolor='rgba(104, 210, 223, 0.15)',
                line=dict(color='rgba(255,255,255,0)'), name='Confidence (80%)'
             ))
             
             # Marker
             f1.add_vline(x=last_hist_date, line_width=3, line_dash="dot", line_color=c_acc)
             f1.add_annotation(x=last_hist_date, y=max(fc_y), text="START", font=dict(color=c_acc, size=10, weight="bold"), yshift=10, showarrow=False)

        style_fig(f1, "Liquidity Forecast (Unified Fan)")
        figures_html.append(pio.to_html(f1, full_html=False, include_plotlyjs='cdn', config={'displayModeBar': False}))

        # --- FIG 2: EFFICIENCY (SANDWICH) ---
        f2 = go.Figure()
        
        # 1. Robust Data Prep (Match Static Logic)
        inflow_raw = self.weekly_data[self.weekly_data['weekly_amount_usd'] > 0].groupby('week')['weekly_amount_usd'].sum()
        outflow_raw = self.weekly_data[self.weekly_data['weekly_amount_usd'] < 0].groupby('week')['weekly_amount_usd'].sum().abs()
        
        # Align to full timeline (Union)
        all_weeks = inflow_raw.index.union(outflow_raw.index)
        
        # Reindex (Fill 0) then Rolling
        inv_weekly = inflow_raw.reindex(all_weeks, fill_value=0).rolling(window=4, min_periods=1).mean()
        outv_weekly = outflow_raw.reindex(all_weeks, fill_value=0).rolling(window=4, min_periods=1).mean()
        
        # UPSAMPLE to DAILY to fix "Brown Triangle" Overlap Artifacts
        # Linear interpolation at daily resolution makes the crossing point precise
        inv_daily = inv_weekly.resample('D').interpolate(method='linear')
        outv_daily = outv_weekly.resample('D').interpolate(method='linear')
        
        # Ensure we don't extrapolate Nans at edges (ffill/bfill small gaps if needed, or just dropna)
        inv_daily = inv_daily.dropna()
        outv_daily = outv_daily.dropna()
        
        # Re-align daily
        common_idx = inv_daily.index.intersection(outv_daily.index)
        inv = inv_daily.loc[common_idx]
        outv = outv_daily.loc[common_idx]
        
        # Calculate Boundaries on High-Res Data
        y_surplus = [max(i, o) for i, o in zip(inv, outv)]
        y_deficit = [max(i, o) for i, o in zip(inv, outv)]
        
        # 2. Plot Traces with Robust 'lines' mode
        # Invisible Base Out
        f2.add_trace(go.Scatter(x=common_idx, y=outv, showlegend=False, mode='lines', line=dict(width=0), hoverinfo='skip'))
        
        # Surplus Fill (Green) -> Fills to Invis Outflow
        f2.add_trace(go.Scatter(
            x=common_idx, y=y_surplus, name='Surplus (Green)', 
            mode='lines', line=dict(width=0), 
            fill='tonexty', fillcolor='rgba(196, 214, 0, 0.5)' # Increased Alpha
        ))
        
        # Invisible Base In
        f2.add_trace(go.Scatter(x=common_idx, y=inv, showlegend=False, mode='lines', line=dict(width=0), hoverinfo='skip'))
        
        # Deficit Fill (Red) -> Fills to Invis Inflow
        f2.add_trace(go.Scatter(
            x=common_idx, y=y_deficit, name='Deficit (Red)', 
            mode='lines', line=dict(width=0), 
            fill='tonexty', fillcolor='rgba(208, 0, 111, 0.3)' # Increased Alpha
        ))
        
        # Visible Lines on Top (High Res for smoothness)
        f2.add_trace(go.Scatter(x=common_idx, y=inv, name='Inflow', line=dict(color=c_pos, width=3)))
        f2.add_trace(go.Scatter(x=common_idx, y=outv, name='Outflow', line=dict(color=c_neg, width=3)))
        
        style_fig(f2, "Efficiency Trend (Gap Analysis)")
        figures_html.append(pio.to_html(f2, full_html=False, include_plotlyjs=False, config={'displayModeBar': False}))

        # --- FIG 3: ENTITIES (BULLET) ---
        f3 = go.Figure()
        if 'entities' in self.forecasts:
            ents, acts, targs, colors = [], [], [], []
            for ent, model in list(self.forecasts['entities'].items())[:5]:
                val = abs(model['1month'].mean())
                ents.append(ent[:15])
                acts.append(val)
                if 'KR10' in ent or 'TW10' in ent:
                    targs.append(val * 0.9)
                    colors.append(c_neg)
                else:
                    targs.append(val * 1.15)
                    colors.append(c_pos)
            
            f3.add_trace(go.Bar(x=ents, y=acts, name='Actual', marker_color=colors, opacity=0.8, text=[f"${x/1e6:.1f}M" for x in acts], textposition='auto'))
            f3.add_trace(go.Scatter(x=ents, y=targs, mode='markers', name='Budget Cap', marker=dict(symbol='line-ew', color=c_text, size=30, line=dict(width=3))))
        
        style_fig(f3, "Top 5 Entities (Burn vs Budget)")
        figures_html.append(pio.to_html(f3, full_html=False, include_plotlyjs=False, config={'displayModeBar': False}))

        # --- FIG 4: CATEGORY (STACKED) ---
        f4 = go.Figure()
        if 'Category' in self.weekly_data.columns:
            cat_pivot_monthly = self.weekly_data.pivot_table(index='week', columns='Category', values='weekly_amount_usd', aggfunc='sum').fillna(0).resample('M').sum().abs()
            top_cats = cat_pivot_monthly.sum().nlargest(5).index.tolist()
            palette = [c_text, c_neg, c_pos, c_acc, '#68D2DF']
            for i, cat in enumerate(top_cats):
                f4.add_trace(go.Scatter(x=cat_pivot_monthly.index, y=cat_pivot_monthly[cat], name=cat, stackgroup='one', mode='none', fillcolor=palette[i%5]))
        
        style_fig(f4, "Category Flow Intensity (Monthly)")
        figures_html.append(pio.to_html(f4, full_html=False, include_plotlyjs=False, config={'displayModeBar': False}))

        # --- FIG 5: RISK (BAR) ---
        f5 = go.Figure()
        if self.anomalies is not None and not self.anomalies.empty:
            risk_monthly = self.anomalies.set_index('posting_date').resample('M')['Amount in USD'].sum().abs()
            f5.add_trace(go.Bar(x=risk_monthly.index, y=risk_monthly.values, name='Risk Value', marker_color=c_neg, opacity=0.7, text=[f"${x/1e6:.1f}M" for x in risk_monthly.values], textposition='auto'))
        else:
            f5.add_trace(go.Scatter(x=[], y=[], name='No Anomalies'))
            
        style_fig(f5, "Value at Risk (Monthly Anomaly Sum)")
        figures_html.append(pio.to_html(f5, full_html=False, include_plotlyjs=False, config={'displayModeBar': False}))

        # --- FIG 6: ACTIONS (TABLE) ---
        f6 = go.Figure()
        ent_names = list(self.forecasts['entities'].keys())
        ent_text = f"Review Caps: {ent_names[0]}/{ent_names[1]}" if len(ent_names) >= 2 else "Review Top Entity Caps"
        f6.add_trace(go.Table(
            header=dict(values=["<b>Action Required</b>", "<b>Priority</b>"], fill_color=c_text, font=dict(color='white', size=12)),
            cells=dict(values=[["Check Liquidity Dip (Week 4)", "Audit Duplicate Payments (High Vol)", ent_text], ["High", "High", "Medium"]], fill_color='white', align='left', height=40)
        ))
        style_fig(f6, "Executive Action Plan")
        figures_html.append(pio.to_html(f6, full_html=False, include_plotlyjs=False, config={'displayModeBar': False}))

        # --- ASSEMBLE HTML ---
        html_template = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AstraZeneca Interactive Command Center</title>
            <style>
                body {{ background-color: {AZ_COLORS['platinum']}; font-family: 'Arial', sans-serif; margin: 0; padding: 20px; }}
                .header {{ text-align: center; margin-bottom: 30px; color: {c_text}; }}
                .header h1 {{ margin: 0; font-size: 32px; font-weight: 800; }}
                .grid-container {{ display: grid; grid-template-columns: 1fr 1fr; gap: 20px; max-width: 1800px; margin: 0 auto; }}
                .card {{ background: white; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); overflow: hidden; border: 1px solid #D0D0D0; }}
                .footer {{ grid-column: 1 / -1; margin-top: 20px; padding: 15px; background: white; border-radius: 8px; border: 2px solid {c_acc}; text-align: center; font-size: 14px; color: {c_text}; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
                .guide-item {{ margin: 0 15px; display: inline-block; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>AstraZeneca Interactive Command Center</h1>
                <p>Executive Cash Flow Intelligence System</p>
            </div>
            
            <div class="grid-container">
                <div class="card">{figures_html[0]}</div>
                <div class="card">{figures_html[1]}</div>
                <div class="card">{figures_html[2]}</div>
                <div class="card">{figures_html[3]}</div>
                <div class="card">{figures_html[4]}</div>
                <div class="card">{figures_html[5]}</div>
                
                <div class="footer">
                    <b>VISUAL GUIDE</b>
                    <span class="guide-item">P1 Fan: Confidence (80%)</span> | 
                    <span class="guide-item">P2 Gap: <span style='color:{c_pos}'>■</span> Surplus  <span style='color:{c_neg}'>■</span> Deficit</span> | 
                    <span class="guide-item">P3 Bullet: <span style='color:{c_pos}'>■</span> OK  <span style='color:{c_neg}'>■</span> Exceeds</span> | 
                    <span class="guide-item">P5 Risk: Total $ Impact</span>
                </div>
            </div>
        </body>
        </html>
        """
        
        with open('AstraZeneca_Interactive_Insights_CommandCenter.html', 'w', encoding='utf-8') as f:
            f.write(html_template)
            
        print("Generated Grid Dashboard: AstraZeneca_Interactive_Insights_CommandCenter.html")

    def generate_insights(self):
        """Generate key insights and recommendations."""
        print("\n=== GENERATING INSIGHTS & RECOMMENDATIONS ===")
        
        insights = {
            'cash_flow_health': '',
            'key_drivers': [],
            'risks': [],
            'recommendations': []
        }
        
        # Analyze overall cash flow health
        if 'Amount in doc. curr.' in self.df.columns:
            total_inflow = self.df[self.df['Amount in doc. curr.'] > 0]['Amount in USD'].sum()
            total_outflow = abs(self.df[self.df['Amount in doc. curr.'] < 0]['Amount in USD'].sum())
            net_position = total_inflow - total_outflow
            
            if net_position > 0:
                insights['cash_flow_health'] = f"Positive net cash position of ${net_position:,.2f}"
            else:
                insights['cash_flow_health'] = f"Negative net cash position of ${abs(net_position):,.2f} - requires attention"
        
        # Identify key drivers
        if 'Category' in self.weekly_data.columns:
            category_impact = self.weekly_data.groupby('Category')['weekly_amount_usd'].sum().sort_values(ascending=False).head(5)
            
            insights['key_drivers'] = [
                f"{category}: ${amount:,.2f}" 
                for category, amount in category_impact.items()
            ]
        
        # Identify risks
        if len(self.anomalies) > 0:
            insights['risks'].append(f"{len(self.anomalies)} anomalous transactions detected requiring review")
        
        # Check for concentration risk
        if 'Category' in self.weekly_data.columns and len(insights['key_drivers']) > 0:
            top_category = insights['key_drivers'][0]
            if 'Amount in doc. curr.' in self.df.columns:
                total_inflow = self.df[self.df['Amount in doc. curr.'] > 0]['Amount in USD'].sum()
                top_category_amount = float(top_category.split('$')[1].replace(',', ''))
                top_category_share = top_category_amount / total_inflow * 100
                if top_category_share > 50:
                    insights['risks'].append(f"High concentration risk: {top_category.split(':')[0]} represents {top_category_share:.1f}% of inflows")
        
        # Generate recommendations
        insights['recommendations'] = [
            "Implement automated monitoring for large transactions",
            "Diversify revenue streams to reduce concentration risk",
            "Review anomalous transactions for potential errors or fraud",
            "Use 6-month forecast for strategic cash planning",
            "Set up weekly cash flow monitoring dashboard"
        ]
        
        # Print insights
        print("CASH FLOW HEALTH:")
        print(f"  {insights['cash_flow_health']}")
        
        print("\nKEY CASH FLOW DRIVERS:")
        for driver in insights['key_drivers']:
            print(f"  • {driver}")
        
        print("\nIDENTIFIED RISKS:")
        for risk in insights['risks']:
            print(f"  ⚠ {risk}")
        
        print("\nRECOMMENDATIONS:")
        for rec in insights['recommendations']:
            print(f"  ✓ {rec}")
        
        return insights

def main():
    """Main function to run the complete cash flow analysis."""
    print("=== ASTRAZENECA CASH FLOW CHALLENGE ANALYSIS ===")
    print("Using Pandas for reliable data processing")
    
    # Initialize analyzer
    analyzer = CashFlowAnalyzer('MATERIALS/Datathon Dataset.xlsx')
    
    # Run analysis pipeline
    if analyzer.load_data():
        analyzer.explore_data()
        analyzer.preprocess_data()
        analyzer.create_forecasts()
        analyzer.detect_anomalies()
        analyzer.generate_visualizations()
        analyzer.generate_interactive_dashboard() # [RESTORED] Interactive Output
        insights = analyzer.generate_insights()
        
        # Answer the specific problem statement questions
        analyzer.answer_suggested_questions()
        
        print("\n=== ANALYSIS COMPLETE ===")
        print("Dashboard saved as 'cash_flow_dashboard.png'")
        print("Ready for presentation submission!")
    else:
        print("Failed to load dataset. Please check the file path.")

if __name__ == "__main__":
    main()