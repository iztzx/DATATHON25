import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Set up AZ color scheme
AZ_COLORS = {
    'mulberry': '#830051',
    'lime_green': '#C4D600',
    'navy': '#003865',
    'graphite': '#3F4444',
    'light_blue': '#68D2DF',
    'magenta': '#D0006F',
    'purple': '#3C1053',
    'gold': '#F0AB00'
}

class CashFlowAnalyzer:
    def __init__(self, dataset_path):
        """Initialize the Cash Flow Analyzer with dataset path."""
        self.dataset_path = dataset_path
        self.df = None
        self.weekly_data = None
        self.forecasts = {}
        self.anomalies = None
        
    def load_data(self):
        """Load the PRE-CLEANED cash flow dataset (CSV)."""
        print("Loading cleaned dataset...")
        try:
            # Load the CSV generated by clean_data.py
            schema = {
                'Amount in USD': float,
                'Net_Amount_USD': float,
                'Week_Num': int,
                'Year': int
            }
            # Low_memory=False to handle mixed types if any, though schema helps
            self.df = pd.read_csv('AstraZeneca_Cleaned_Processed_Data.csv', dtype=schema, parse_dates=['posting_date', 'week'])
            
            print(f"Cleaned Dataset loaded. Shape: {self.df.shape}")
            
            # Verify critical columns exist (The "Theory" inputs)
            required_cols = ['Net_Amount_USD', 'Activity', 'date', 'Category']
            # Note: 'date' might be 'posting_date' in CSV
            if 'posting_date' in self.df.columns:
                self.df['date'] = self.df['posting_date']
            
            
            # PURE CSV Strategy: No external Excel dependencies
            # We will derive "Cumulative Position" from Flows if needed, 
            # rather than relying on a potentially disconnected Balance sheet.
            self.df_balance = None 
            self.df_cat_link = None 
            self.df_country = None
            
            return True
        except FileNotFoundError:
            print("Error: 'AstraZeneca_Cleaned_Processed_Data.csv' not found. Run clean_data.py first!")
            return False
        except Exception as e:
            print(f"Error loading dataset: {e}")
            return False
    
    def explore_data(self):
        """Explore and understand the dataset structure."""
        print("\n=== DATA EXPLORATION ===")
        
        # Basic info
        print(f"Dataset shape: {self.df.shape}")
        if 'Name' in self.df.columns:
            print(f"Number of unique entities: {self.df['Name'].nunique()}")
        if 'Pstng Date' in self.df.columns:
            print(f"Date range: {self.df['Pstng Date'].min()} to {self.df['Pstng Date'].max()}")
        
        # Cash flow categories
        if 'Category' in self.df.columns:
            print(f"\nCash flow categories:")
            category_counts = self.df['Category'].value_counts().head(10)
            print(category_counts)
        
        # Currency distribution
        if 'Curr.' in self.df.columns:
            print(f"\nCurrency distribution:")
            currency_counts = self.df['Curr.'].value_counts()
            print(currency_counts)
        
        # Basic statistics for amounts
        if 'Amount in doc. curr.' in self.df.columns:
            print(f"\nAmount statistics (in document currency):")
            amount_stats = self.df['Amount in doc. curr.'].describe()
            print(amount_stats)
        
        return True
    
    def preprocess_data(self):
        """Preprocess data using pre-cleaned attributes (CSV Source)."""
        print("\n=== DATA PREPROCESSING (Optimized) ===")
        
        # 1. Verify Activity Column (Crucial for Theory)
        if 'Activity' in self.df.columns:
             print("  • Activity column verified (Operating/Investing/Financing).")
        else:
             print("  • Warning: 'Activity' column missing. Check clean_data.py.")

        # 2. Date/Time checks
        # 'month' might be missing if only 'week' was in CSV or not parsed as date
        if 'month' not in self.df.columns and 'posting_date' in self.df.columns:
            self.df['month'] = self.df['posting_date'].dt.to_period('M').dt.start_time
            
        # 3. Aggregation (Weekly)
        # We group by Week, Category, Activity to maintain granularity for Forecasts
        group_cols = ['week']
        if 'Category' in self.df.columns:
            group_cols.append('Category')
        if 'Activity' in self.df.columns:
            group_cols.append('Activity') # Critical for DSS
            
        # Use 'Net_Amount_USD' if possible (the "Theoretically Correct" signed amount)
        # Fallback to 'Amount in USD' (which might be absolute)
        val_col = 'Amount in USD'
        if 'Net_Amount_USD' in self.df.columns:
             val_col = 'Net_Amount_USD'
        else:
             print("  • Note: Net_Amount_USD not found, using Amount in USD.")
        
        # Aggregation Dictionary
        agg_dict = {
            val_col: 'sum',
        }
        
        self.weekly_data = self.df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Rename for internal consistency with Forecast engine
        self.weekly_data = self.weekly_data.rename(columns={
            val_col: 'weekly_amount_usd'
        })
        
        # Sort
        self.weekly_data = self.weekly_data.sort_values('week')
        
        print(f"Weekly data aggregated. Records: {len(self.weekly_data)}")
        return True

    def preprocess_data_deprecated(self):
        """Preprocess data for analysis using Pandas."""
        print("\n=== DATA PREPROCESSING ===")
        
        # 1. Merge Category Linkage (Operating / Investing / Financing)
        if self.df_cat_link is not None and 'Category' in self.df.columns:
            # Assume Linkage sheet has 'Category' and 'Cash Flow Type' or similar
            # Let's check columns for robustness (Hardcoding based on standard template expectation)
            # Typically: 'Category', 'Activity' or 'Type'
            print("Merging Category Linkage...")
            
            # Standardize names for merge
            if 'Category' in self.df_cat_link.columns:
                # Normalize strings (strip whitespace, consistent case)
                self.df['Category_Clean'] = self.df['Category'].astype(str).str.strip()
                self.df_cat_link['Category_Clean'] = self.df_cat_link['Category'].astype(str).str.strip()
                
                self.df = pd.merge(self.df, self.df_cat_link, left_on='Category_Clean', right_on='Category_Clean', how='left', suffixes=('', '_link'))
                
                # Identify the activity column (usually the 2nd column if not named standardly)
                # We rename it to 'Activity' for consistency
                new_cols = [c for c in self.df.columns if c not in self.df_cat_link.columns or c == 'Category']
                added_cols = [c for c in self.df.columns if c not in new_cols]
                
                if added_cols:
                    activity_col = added_cols[0] # Take the first new column as Activity
                    self.df['Activity'] = self.df[activity_col]
                    print(f"Mapped Categories to Activity using column: {activity_col}")
            else:
                 print("Warning: 'Category' column not found in Linkage sheet.")

        # 2. Merge Country Mapping (if useful later)
        if self.df_country is not None and 'Name' in self.df.columns:
             pass # Logic for country merge if needed, skipping for now to focus on Cash Flow

        # Convert posting date to datetime
        if 'Pstng Date' in self.df.columns:
            self.df['posting_date'] = pd.to_datetime(self.df['Pstng Date'], errors='coerce')
            # Drop rows with invalid dates
            self.df = self.df.dropna(subset=['posting_date'])
        
        # Add week and month columns for time series analysis
        self.df['week'] = self.df['posting_date'].dt.to_period('W').dt.start_time
        self.df['month'] = self.df['posting_date'].dt.to_period('M').dt.start_time
        
        # Create cash flow direction (inflow/outflow)
        if 'Amount in doc. curr.' in self.df.columns:
            self.df['cash_flow_direction'] = np.where(
                self.df['Amount in doc. curr.'] > 0, 'Inflow', 'Outflow'
            )
        
        # Aggregate to weekly level
        group_cols = ['week']
        if 'Category' in self.df.columns:
            group_cols.append('Category')
        if 'Activity' in self.df.columns:
            group_cols.append('Activity') # Group by Activity too
            
        group_cols.append('cash_flow_direction')
        
        agg_dict = {}
        if 'Amount in doc. curr.' in self.df.columns:
            agg_dict['Amount in doc. curr.'] = 'sum'
        if 'Amount in USD' in self.df.columns:
            agg_dict['Amount in USD'] = 'sum'
        if 'DocumentNo' in self.df.columns:
            agg_dict['DocumentNo'] = 'count'
        
        self.weekly_data = self.df.groupby(group_cols).agg(agg_dict).reset_index()
        
        # Rename columns for clarity
        if 'Amount in doc. curr.' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'Amount in doc. curr.': 'weekly_amount'})
        if 'Amount in USD' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'Amount in USD': 'weekly_amount_usd'})
        if 'DocumentNo' in agg_dict:
            self.weekly_data = self.weekly_data.rename(columns={'DocumentNo': 'transaction_count'})
        
        # Sort by week
        self.weekly_data = self.weekly_data.sort_values('week')
        
        # --- EXPORT CLEANED DATA ---
        export_path = 'AstraZeneca_Cleaned_Processed_Data.csv'
        print(f"\n[EXPORTing] Saving cleaned dataset to '{export_path}'...")
        # Select key columns for the user
        export_cols = [c for c in self.df.columns if c in [
            'Name', 'DocumentNo', 'posting_date', 'week', 'Category', 'Category_Clean', 
            'Activity', 'Amount in doc. curr.', 'Amount in USD', 'cash_flow_direction'
        ]]
        self.df[export_cols].to_csv(export_path, index=False)
        print(f"  • Export complete. Rows: {len(self.df)}")
        
        print(f"Weekly data created with {len(self.weekly_data)} records")
        return True
    
    def _generate_forecast_model(self, series, name="Total"):
        """
        Generate forecast model for a given time series.
        Returns dictionary with forecast data and metrics.
        """
        # Guard clause for empty series
        if series.empty:
            print(f"Warning: No data for {name}. Returning empty forecast.")
            empty_series = pd.Series([], dtype=float)
            return {
                'name': name,
                '1month': empty_series,
                '6month': empty_series,
                'historical': empty_series,
                'es_values': empty_series,
                'trend': 0,
                'mae': 0,
                'rmse': 0
            }

        # Calculate trend using linear regression on recent data
        recent_weeks = min(12, len(series))
        
        
        # Optimize parameters (Cleaned up duplications)
        alpha, beta = self._optimize_holt_parameters(series)
        
        try:
            level = series.iloc[0]
        except IndexError:
            print(f"\nCRITICAL ERROR in _generate_forecast_model for '{name}':")
            print(f"  Type: {type(series)}")
            print(f"  Shape: {series.shape}")
            print(f"  Empty: {series.empty}")
            print(f"  Head: {series.head()}")
            # Return empty to avoid crash
            return {
                'name': name,
                '1month': pd.Series([], dtype=float),
                '6month': pd.Series([], dtype=float),
                'historical': series,
                'es_values': pd.Series([], dtype=float),
                'trend': 0, 'mae': 0, 'rmse': 0
            }
            
        trend = 0
        
        es_values = []
        
        for value in series.values:
            new_level = alpha * value + (1 - alpha) * (level + trend)
            new_trend = beta * (new_level - level) + (1 - beta) * trend
            es_values.append(new_level)
            level = new_level
            trend = new_trend
            
        last_date = series.index[-1]
        last_level = level
        last_trend = trend
        
        # 1-month forecast (4 weeks)
        forecast_1month_dates = pd.date_range(start=last_date + timedelta(weeks=1), periods=4, freq='W')
        forecast_1month_values = []
        historical_volatility = series.std() if len(series) > 1 else 0
        
        for i in range(4):
            trended_value = last_level + (i + 1) * last_trend
            variation = np.random.normal(0, historical_volatility * 0.3)
            forecast_1month_values.append(trended_value + variation)
            
        forecast_1month = pd.Series(forecast_1month_values, index=forecast_1month_dates)
        
        # 6-month forecast (24 weeks)
        forecast_6month_dates = pd.date_range(start=last_date + timedelta(weeks=1), periods=24, freq='W')
        forecast_6month_values = []
        damping_factor = 0.95
        
        for i in range(24):
            damped_trend = last_trend * (damping_factor ** i)
            trended_value = last_level + (i + 1) * damped_trend
            variation_std = historical_volatility * 0.3 * (1 - i/24)
            variation = np.random.normal(0, variation_std)
            forecast_6month_values.append(trended_value + variation)
            
        forecast_6month = pd.Series(forecast_6month_values, index=forecast_6month_dates)
        
        # Accuracy metrics
        mae, rmse = 0, 0
        if len(series) > 8:
            test_actual = series.iloc[-8:]
            # Simple moving average as baseline for error calculation
            ma_baseline = series.rolling(window=4).mean().shift(1)
            test_forecast = ma_baseline.iloc[-8:]
            
            # Align
            common_idx = test_actual.index.intersection(test_forecast.index)
            if len(common_idx) > 0:
                mae = mean_absolute_error(test_actual[common_idx], test_forecast[common_idx])
                rmse = np.sqrt(mean_squared_error(test_actual[common_idx], test_forecast[common_idx]))

        return {
            'name': name,
            '1month': forecast_1month,
            '6month': forecast_6month,
            'historical': series,
            'es_values': pd.Series(es_values, index=series.index),
            'trend': last_trend,
            'mae': mae,
            'rmse': rmse
        }

    def create_forecasts(self):
        """Create time series forecasts for cash flow, activities, and ending balance."""
        print("\n=== TIME SERIES FORECASTING & CLOSING BALANCE ===")
        
        # 1. Total Cash Flow Forecast
        weekly_totals = self.weekly_data.groupby('week')['weekly_amount_usd'].sum()
        print("Generating forecast for Total Net Cash Flow...")
        self.forecasts['total'] = self._generate_forecast_model(weekly_totals, "Total Net Cash Flow")
        
        # Backward compatibility
        self.forecasts['historical'] = self.forecasts['total']['historical']
        self.forecasts['1month'] = self.forecasts['total']['1month']
        self.forecasts['6month'] = self.forecasts['total']['6month']
        self.forecasts['es_values'] = self.forecasts['total']['es_values']
        
        # 2. Activity-Based Forecasts (Operating / Investing / Financing)
        print("Generating forecasts by Activity (Operating, Investing, Financing)...")
        self.forecasts['activities'] = {}
        if 'Activity' in self.weekly_data.columns:
            activities = self.weekly_data['Activity'].unique()
            for act in activities:
                # Need to handle NaN activity
                if pd.isna(act): continue
                
                print(f"  - Forecasting for: {act}")
                act_data = self.weekly_data[self.weekly_data['Activity'] == act]
                act_weekly = act_data.groupby('week')['weekly_amount_usd'].sum()
                act_weekly = act_weekly.reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['activities'][str(act)] = self._generate_forecast_model(act_weekly, str(act))
                
        # 3. Forecast Ending Cash Balance
        # We need the LAST ACTUAL closing balance from self.df_balance
        print("Projecting Ending Cash Balances...")
        self.forecasts['balance'] = {}
        
        # 3. Forecast Cumulative Net Position (Relative Liquidity)
        # Since we removed external Balance sheet, we track Cumulative Flow Trend
        print("Projecting Cumulative Net Cash Position...")
        self.forecasts['balance'] = {}
        
        # Start from 0 (Relative Change)
        last_balance = 0
        self.forecasts['balance']['last_actual'] = last_balance # Proxy
        
        # 1-Month Cumulative Forecast
        fc_1m_flows = self.forecasts['total']['1month']
        fc_1m_bal = []
        running_bal = last_balance
        for flow in fc_1m_flows.values:
            running_bal += flow
            fc_1m_bal.append(running_bal)
        self.forecasts['balance']['1month'] = pd.Series(fc_1m_bal, index=fc_1m_flows.index)
        
        # 6-Month Cumulative Forecast
        fc_6m_flows = self.forecasts['total']['6month']
        fc_6m_bal = []
        running_bal = last_balance
        for flow in fc_6m_flows.values:
            running_bal += flow
            fc_6m_bal.append(running_bal)
        self.forecasts['balance']['6month'] = pd.Series(fc_6m_bal, index=fc_6m_flows.index)
        
        print("  - Generated Relative Liquidity Projection (Cumulative Flow)")

        # 4. Category Forecasts (Top Drivers)
        print("Generating forecasts for Top Categories...")
        self.forecasts['categories'] = {}
        
        if 'Category' in self.weekly_data.columns:
            # Identify top 2 categories by volume
            cat_volumes = self.weekly_data.groupby('Category')['weekly_amount_usd'].apply(lambda x: x.abs().sum())
            top_categories = cat_volumes.nlargest(2).index.tolist()
            
            for cat in top_categories:
                print(f"  - Forecasting for Category: {cat}")
                cat_data = self.weekly_data[self.weekly_data['Category'] == cat]
                cat_weekly = cat_data.groupby('week')['weekly_amount_usd'].sum()
                # Reindex
                cat_weekly = cat_weekly.reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['categories'][cat] = self._generate_forecast_model(cat_weekly, cat)

        # 5. Entity Forecasts (Top Spenders) - [NEW] Extracting More Value
        print("Generating forecasts for Top Entities...")
        self.forecasts['entities'] = {}
        
        if 'Name' in self.df.columns:
            # Aggregate weekly by Name
            # We need to rebuild weekly agg for Name since self.weekly_data might not have it grouped
            # Let's check self.weekly_data or go back to self.df
            # self.weekly_data grouped by [week, Category, Activity]. Name is lost.
            # Go back to self.df
            
            # Top 2 Spenders (Outflow)
            outflows = self.df[self.df['Amount in USD'] < 0]
            top_entities = outflows.groupby('Name')['Amount in USD'].sum().abs().nlargest(2).index.tolist()
            
            for ent in top_entities:
                print(f"  - Forecasting for Entity: {ent}")
                # Filter and Agg
                ent_data = self.df[self.df['Name'] == ent]
                if 'week' not in ent_data.columns:
                     ent_data['week'] = pd.to_datetime(ent_data['posting_date']).dt.to_period('W').dt.start_time
                
                ent_weekly = ent_data.groupby('week')['Amount in USD'].sum()
                ent_weekly = ent_weekly.sort_index().reindex(weekly_totals.index, fill_value=0)
                
                self.forecasts['entities'][ent] = self._generate_forecast_model(ent_weekly, ent)

        # 5. EXPORT FORECAST DATA (ESS Ready)
        print("Exporting Forecast Results to CSV...")
        forecast_rows = []
        
        # Helper to unpack forecast object
        def unpack_forecast(model_out, fc_type):
            # 1-Month Forecast
            if '1month' in model_out and not model_out['1month'].empty:
                for date, val in model_out['1month'].items():
                    forecast_rows.append({
                        'Forecast_Type': fc_type,
                        'Model_Name': model_out['name'],
                        'Date': date,
                        'Value_USD': val,
                        'Scenario': 'Base Case',
                        'Horizon': 'Short-Term (1M)'
                    })
            # 6-Month Forecast
            if '6month' in model_out and not model_out['6month'].empty:
                for date, val in model_out['6month'].items():
                    forecast_rows.append({
                        'Forecast_Type': fc_type,
                        'Model_Name': model_out['name'],
                        'Date': date,
                        'Value_USD': val,
                        'Scenario': 'Base Case',
                        'Horizon': 'Medium-Term (6M)'
                    })
        
        # Unpack Total
        if 'total' in self.forecasts:
            unpack_forecast(self.forecasts['total'], 'Total Net Flow')
            
        # Unpack Activities
        if 'activities' in self.forecasts:
            for act_name, model in self.forecasts['activities'].items():
                unpack_forecast(model, f"Activity: {act_name}")
                
        # Unpack Categories
        if 'categories' in self.forecasts:
            for cat_name, model in self.forecasts['categories'].items():
                unpack_forecast(model, f"Category: {cat_name}")
                
        # Save to CSV
        if forecast_rows:
            pd.DataFrame(forecast_rows).to_csv('AstraZeneca_Forecast_Results.csv', index=False)
            print(f"  • Forecasts exported: {len(forecast_rows)} rows")
                
        return True

    def answer_suggested_questions(self):
        """
        Generate a Decision Support System (DSS) Executive Report.
        Answers AstraZeneca's key questions with data-driven strategic insights.
        """
        print("\n" + "#"*70)
        print("   ASTRAZENECA EXECUTIVE DECISION SUPPORT SYSTEM (DSS) REPORT")
        print("   CLASSIFICATION: STRICTLY CONFIDENTIAL / COMPANY RESTRICTED")
        print("#"*70)
        
        # ---------------------------------------------------------
        # SECTION 1: Strategic Forecast (1-Month & 6-Month)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 1. LIQUIDITY FORECASTING & STRATEGY")
        print("="*70)
        
        total_fc = self.forecasts['total']
        is_growth = total_fc['trend'] > 0
        trend_status = "POSITIVE GROWTH" if is_growth else "CONTRACTION ALERT"
        
        print(f"\n[SHORT-TERM] 1-Month Outlook: {trend_status}")
        print(f"  • Expected Net Position (4-Week Sum): ${total_fc['1month'].sum()/1e6:.2f}M")
        print(f"  • Weekly Trend Slope: ${total_fc['trend']/1e6:.2f}M per week")
        print(f"  • Model Confidence (RMSE): +/- ${total_fc['rmse']/1e6:.2f}M")
        
        print(f"\n[MEDIUM-TERM] 6-Month Trajectory")
        if not total_fc['6month'].empty:
            end_bal = total_fc['6month'].iloc[-1]
            print(f"  • Projected Weekly Flow by Month 6: ${end_bal/1e6:.2f}M")
            print(f"  • Sustainability Score: {'High' if end_bal > 0 else 'Medium-Risk'}")
        else:
            print("  • Projection data unavailable (insufficient history)")
        
        print("\n>>> DECISION SUPPORT: RECOMMENDED ACTIONS")
        if total_fc['1month'].sum() < 0:
             print("  [ACTION] TRIGGER LIQUIDITY CONTINGENCY: Short-term flows are projected negative.")
             print("  [ACTION] REVIEW: Delay discretionary payments scheduled for weeks 3-4.")
        else:
             print("  [ACTION] INVEST SURPLUS: Excess liquidity identified. Evaluate short-term investment instruments.")
             
        # ---------------------------------------------------------
        # SECTION 2: Anomaly Triage (Risk & Control)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 2. RISK & CONTROL: ANOMALY TRIAGE")
        print("="*70)
        
        if self.anomalies is not None and not self.anomalies.empty:
            print(f"\n[ALERT] {len(self.anomalies)} Transactions Flagged for Review")
            
            # Group by type
            summary = self.anomalies['anomaly_type'].value_counts()
            for atype, count in summary.items():
                print(f"  • {atype}: {count} items")
            
            print("\n>>> HIGH PRIORITY INVESTIGATION LIST (Top Risks)")
            high_risk = self.anomalies.head(5)
            print(f"{'Date':<12} | {'DocNo':<10} | {'Type':<20} | {'Amount ($)':>12} | {'Category'}")
            print("-" * 80)
            
            for idx, row in high_risk.iterrows():
                d = str(row['posting_date'].date()) if 'posting_date' in row else 'N/A'
                doc = str(row.get('DocumentNo', 'N/A'))
                atype = str(row.get('anomaly_type', 'Unknown'))
                amt = f"{row.get('Amount in USD', 0):,.2f}"
                cat = str(row.get('Category', 'N/A'))[:20]
                print(f"{d:<12} | {doc:<10} | {atype:<20} | {amt:>12} | {cat}")
                
            print("\n>>> DECISION SUPPORT: INVESTIGATION PROTOCOL")
            print("  [ACTION - DUPLICATES]: Verify if 'Potential Duplicates' share Invoice References in source system.")
            print("  [ACTION - ROUND NUMBERS]: Request supporting documentation for large round-number manual entries.")
            print("  [ACTION - SPIKES]: Confirm if statistical outliers align with known strategic initiatives (M&A, Capex).")
        else:
            print("\n[STATUS] No material anomalies detected. Standard monitoring active.")

        # ---------------------------------------------------------
        # SECTION 3: Driver Analysis (Business Logic)
        # ---------------------------------------------------------
        print("\n" + "="*70)
        print(" 3. STRATEGIC FINANCIAL METRICS & DRIVERS")
        print("="*70)
        
        # A. CALCULATE METRICS (More Theory)
        # Filter for Operating Activity
        if 'Activity' in self.weekly_data.columns:
            op_data = self.weekly_data[self.weekly_data['Activity'] == 'Operating']
            
            # 1. Cash Burn Rate (Avg Weekly Outflow)
            # Filter where amount < 0
            op_outflows = op_data[op_data['weekly_amount_usd'] < 0]['weekly_amount_usd']
            avg_burn_weekly = op_outflows.mean() if not op_outflows.empty else 0
            
            # 2. Operating Efficiency Ratio (Inflow / Outflow)
            op_inflows = op_data[op_data['weekly_amount_usd'] > 0]['weekly_amount_usd'].sum()
            op_out_total = op_data[op_data['weekly_amount_usd'] < 0]['weekly_amount_usd'].abs().sum()
            efficiency_ratio = op_inflows / op_out_total if op_out_total != 0 else 0
            
            # 3. Liquidity Coverage (Runway)
            last_bal = self.forecasts['balance']['last_actual'] if 'balance' in self.forecasts and 'last_actual' in self.forecasts['balance'] else 0
            runway_weeks = (last_bal / abs(avg_burn_weekly)) if avg_burn_weekly != 0 else 999
            
            print(f"\n[KEY PERFORMANCE INDICATORS (KPIs)]")
            print(f"  • Operating Cash Burn: ${abs(avg_burn_weekly)/1e6:.2f}M / week")
            print(f"  • Operating Efficiency: {efficiency_ratio:.2f}x (Target > 1.0)")
            print(f"    (For every $1 out, company generates ${efficiency_ratio:.2f})")
            
            if runway_weeks < 12:
                 print(f"  • RUNWAY ALERT: Cash balance covers only {runway_weeks:.1f} weeks of operating burn.")
            else:
                 print(f"  • Liquidity Health: robust coverage detected ({runway_weeks:.1f} weeks runway).")
                 
        if 'categories' in self.forecasts:
            print("\n[KEY CATEGORY DRIVERS]")
            for cat, data in self.forecasts['categories'].items():
                start_val = data['historical'].tail(4).mean()
                end_val = data['1month'].mean()
                pct_change = ((end_val - start_val) / start_val) * 100 if start_val != 0 else 0
                
                direction = "IMPROVING" if (end_val > start_val and end_val > 0) else "DECLINING"
                print(f"  • {cat}: {direction} ({pct_change:+.1f}%) -> Avg Next Month: ${end_val/1e6:.1f}M")

        print("\n[METHODOLOGY NOTE]")
        print("  • Model: Optimized Holt-Winters Exponential Smoothing (Auto-Tuned Alpha/Beta).")
        print("  • Damping: applied to long-term forecasts to prevent variance explosion.")
        print("  • Scenarios: Volatility-adjusted simulations included in visual dashboard.")
        print("#"*70 + "\n")
        
        return True

    
    def _optimize_holt_parameters(self, series):
        """
        Grid search to find optimal alpha (level) and beta (trend) parameters.
        Returns best params and the associated error.
        """
        best_alpha, best_beta = 0.3, 0.2
        best_rmse = float('inf')
        
        # Grid search range
        alphas = [0.1, 0.3, 0.5, 0.7, 0.9]
        betas = [0.1, 0.2, 0.3, 0.4]
        
        # Split for validation (last 4 weeks as validation set)
        if len(series) < 8:
            return best_alpha, best_beta
            
        train = series.iloc[:-4]
        valid = series.iloc[-4:]
        
        if train.empty:
            return best_alpha, best_beta
            
        try:
           # Dry run to check index access
           _ = train.iloc[0]
        except:
           return best_alpha, best_beta
        
        for a in alphas:
            for b in betas:
                # Run Holt's on train
                level = train.iloc[0]
                trend = 0
                preds = []
                
                # Fit
                for val in train.values:
                    last_level = level
                    level = a * val + (1 - a) * (last_level + trend)
                    trend = b * (level - last_level) + (1 - b) * trend
                
                # Forecast
                last_level = level
                last_trend = trend
                for i in range(4):
                     preds.append(last_level + (i+1)*last_trend)
                
                # Evaluate
                try:
                    rmse = np.sqrt(mean_squared_error(valid, preds))
                    if rmse < best_rmse:
                        best_rmse = rmse
                        best_alpha = a
                        best_beta = b
                except:
                    continue
                    
        return best_alpha, best_beta

    def detect_anomalies(self):
        """
        Detect anomalies using a Hybrid approach: 
        1. Statistical (Isolation Forest)
        2. Rule-based (Accounting irregularities)
        """
        print("\n=== HYBRID ANOMALY DETECTION (DSS MODULE) ===")
        
        # 1. Statistical Detection (Isolation Forest)
        # ---------------------------------------------
        feature_cols = []
        if 'Amount in doc. curr.' in self.df.columns:
            feature_cols.append('Amount in doc. curr.')
        if 'Amount in USD' in self.df.columns:
            feature_cols.append('Amount in USD')
        
        features = self.df[feature_cols].copy()
        
        if 'Category' in self.df.columns:
            category_dummies = pd.get_dummies(self.df['Category'], prefix='category')
            features = pd.concat([features, category_dummies], axis=1)
        
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features.fillna(0))
        
        iso_forest = IsolationForest(contamination=0.03, random_state=42) # Reduced to 3% to focus on high priority
        stat_anomaly_mask = iso_forest.fit_predict(features_scaled) == -1
        
        # Initialize anomaly column
        self.df['anomaly_type'] = None
        self.df.loc[stat_anomaly_mask, 'anomaly_type'] = 'Statistical Outlier'
        
        # 2. Rule-Based Detection (Accounting Logic)
        # ---------------------------------------------
        
        # Rule A: Potential Duplicates (Same Amount, Same Date, Different Doc)
        print("Scaning for duplicate payments...")
        if all(col in self.df.columns for col in ['Amount in USD', 'posting_date', 'Category']):
            # Find duplicates based on Amount, Date, Category (ignoring DocumentNo)
            # We filter for non-zero amounts
            mask_real = self.df['Amount in USD'].abs() > 0.01
            
            dupe_cols = ['Amount in USD', 'posting_date', 'Category']
            duplicates = self.df[mask_real].duplicated(subset=dupe_cols, keep=False)
            
            # Update anomaly type (prioritize specific rules over statistical)
            self.df.loc[duplicates & mask_real, 'anomaly_type'] = 'Potential Duplicate'

        # Rule B: Round Number Checks (e.g. 100000.00) - often manual
        print("Scanning for 'Round Number' manual entries...")
        if 'Amount in USD' in self.df.columns:
            # Check if amount is round thousand
            is_round = (self.df['Amount in USD'].abs() % 1000 == 0) & (self.df['Amount in USD'].abs() > 1000)
            self.df.loc[is_round & (self.df['anomaly_type'].isna()), 'anomaly_type'] = 'Round Number (Manual Risk)'

        # Rule C: Weekend Postings (Unusual for corporate)
        print("Scanning for Weekend transactions...")
        if 'posting_date' in self.df.columns:
            is_weekend = self.df['posting_date'].dt.dayofweek >= 5
            # Only trigger if not already flagged
            self.df.loc[is_weekend & (self.df['anomaly_type'].isna()), 'anomaly_type'] = 'Weekend Posting'

        # Consolidate
        self.anomalies = self.df[self.df['anomaly_type'].notna()].copy()
        
        # Prioritize anomalies (Risk Scoring)
        risk_map = {
            'Potential Duplicate': 3,   # High
            'Statistical Outlier': 2,   # Medium
            'Weekend Posting': 1,       # Low risk but notable
            'Round Number (Manual Risk)': 1
        }
        self.anomalies['risk_score'] = self.anomalies['anomaly_type'].map(risk_map)
        self.anomalies = self.anomalies.sort_values(by=['risk_score', 'Amount in USD'], ascending=[False, False])
        
        count = len(self.anomalies)
        print(f"DSS Alert: Detected {count} irregularities requiring review.")
        print(f" Breakdown: {self.anomalies['anomaly_type'].value_counts().to_dict()}")
        
        return True
    
    def generate_visualizations(self):
        """Generate enhanced visualizations using AZ color scheme."""
        print("\n=== GENERATING ENHANCED VISUALIZATIONS ===")
        
        # Set up matplotlib with AZ colors and better styling
        plt.style.use('default')
        sns.set_palette([AZ_COLORS['navy'], AZ_COLORS['mulberry'], AZ_COLORS['lime_green'], AZ_COLORS['gold']])
        
        # Create larger, less cramped subplots
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        fig.suptitle('AstraZeneca Cash Flow Analysis Dashboard', fontsize=20, fontweight='bold', color=AZ_COLORS['navy'])
        
        # Adjust spacing between subplots
        plt.subplots_adjust(hspace=0.3, wspace=0.25)
        
    def generate_visualizations(self):
        """
        Generate a static executive dashboard (v2).
        Focus: Denser information, Entity Analysis, and clearer insights.
        """
        print("\n=== GENERATING EXECUTIVE DASHBOARD (V2) ===")
        
        # Use a style for better aesthetics
        try:
            plt.style.use('seaborn-v0_8-darkgrid')
        except:
            plt.style.use('ggplot') # Fallback
            
        # AstraZen Colors
        AZ_COLORS = {
            'navy': '#003865',
            'magenta': '#D7004B',
            'lime_green': '#84BD00',
            'gold': '#F0AB00',
            'light_blue': '#68D2DF'
        }
        
        # Create 3x2 Grid
        fig, axes = plt.subplots(3, 2, figsize=(18, 14))
        plt.suptitle("AstraZeneca Executive Cash Flow Dashboard", fontsize=20, fontweight='bold', color=AZ_COLORS['navy'])
        
        # --- PANEL 1: LIQUIDITY PROJECTION (Dual Axis) ---
        ax1 = axes[0, 0]
        # Plot 1: Net Flow Bar
        # Agg net flow
        weekly_net = self.weekly_data.groupby('week')['weekly_amount_usd'].sum()
        
        ax1.bar(weekly_net.index, weekly_net.values, color=AZ_COLORS['light_blue'], label='Net Weekly Flow', alpha=0.6)
        
        # Plot 2: Balance Line (Secondary Axis)
        ax1b = ax1.twinx()
        if 'balance' in self.forecasts and 'last_actual' in self.forecasts['balance']:
             last_bal = self.forecasts['balance']['last_actual']
             if '6month' in self.forecasts['balance']:
                 bal_fc = self.forecasts['balance']['6month']
                 ax1b.plot(bal_fc.index, bal_fc.values, color=AZ_COLORS['navy'], linewidth=3, label='Proj. Balance')
                 # Add reference line (Safety Threshold)
                 ax1b.axhline(y=last_bal*0.2, color='red', linestyle='--', label='Liquidity Risk Floor')
             
        ax1.set_title("Liquidity Outlook: Flow vs Balance", fontweight='bold', color=AZ_COLORS['navy'])
        ax1.set_ylabel("Net Flow (USD)")
        ax1b.set_ylabel("Cash Balance (USD)")
        ax1.grid(False) # Turn off grid for bars to avoid clutter
        
        # --- PANEL 2: STRATEGIC ACTIVITY SPLIT (Stacked Area) ---
        ax2 = axes[0, 1]
        if 'Activity' in self.weekly_data.columns:
            # Pivot: Date x Activity -> Sum Amount
            act_pivot = self.weekly_data.pivot_table(index='week', columns='Activity', values='weekly_amount_usd', aggfunc='sum').fillna(0)
            # Plot
            if 'Operating' in act_pivot.columns:
                ax2.plot(act_pivot.index, act_pivot['Operating'], color=AZ_COLORS['lime_green'], label='Operating', linewidth=2)
            if 'Investing' in act_pivot.columns:
                ax2.plot(act_pivot.index, act_pivot['Investing'], color=AZ_COLORS['magenta'], label='Investing', linewidth=2)
            if 'Financing' in act_pivot.columns:
                ax2.plot(act_pivot.index, act_pivot['Financing'], color=AZ_COLORS['gold'], label='Financing', linewidth=2)
            
            ax2.set_title("Operational Efficiency Trend (Activity)", fontweight='bold', color=AZ_COLORS['navy'])
            ax2.legend(loc='upper left')
            ax2.axhline(0, color='black', linewidth=1)
        
        # --- PANEL 3: ENTITY ANALYSIS (Who is spending?) ---
        # "Use everything in the CSV" -> Using 'Name' column
        ax3 = axes[1, 0]
        if 'Name' in self.df.columns:
            # Top 5 Entities by Outflow (Spend)
            outflows = self.df[self.df['Amount in USD'] < 0]
            top_spenders = outflows.groupby('Name')['Amount in USD'].sum().abs().nlargest(5)
            
            # Horizontal Bar
            y_pos = np.arange(len(top_spenders))
            ax3.barh(y_pos, top_spenders.values, color=AZ_COLORS['magenta'])
            ax3.set_yticks(y_pos)
            ax3.set_yticklabels(top_spenders.index)
            ax3.invert_yaxis() # Top spender at top
            ax3.set_title("Top 5 Entities by Cash Burn (Outflow)", fontweight='bold', color=AZ_COLORS['navy'])
            
            # Add labels
            for i, v in enumerate(top_spenders.values):
                ax3.text(v/2, i, f"${v/1e6:.1f}M", color='white', fontweight='bold', va='center')
        else:
             ax3.text(0.5, 0.5, "Entity Data (Name) Not Available", ha='center')

        # --- PANEL 4: CATEGORY HEATMAP (Top 5) ---
        ax4 = axes[1, 1]
        if 'Category' in self.weekly_data.columns:
            # Get Top 5 Cats
            top_cats = self.weekly_data.groupby('Category')['weekly_amount_usd'].sum().abs().nlargest(5).index
            # Filter
            heat_data = self.weekly_data[self.weekly_data['Category'].isin(top_cats)]
            # Pivot
            heat_pivot = heat_data.pivot_table(index='Category', columns='week', values='weekly_amount_usd', aggfunc='sum')
            
            # Use Imshow
            cax = ax4.imshow(heat_pivot, cmap='RdYlGn', aspect='auto') # Red=Outflow, Green=Inflow
            ax4.set_yticks(np.arange(len(heat_pivot.index)))
            ax4.set_yticklabels(heat_pivot.index)
            # Reduce X ticks to avoid clutter (show every 4th)
            xticks = np.arange(0, len(heat_pivot.columns), 4)
            ax4.set_xticks(xticks)
            # Format date strings safely
            xlabels = [str(heat_pivot.columns[i])[:10] for i in xticks]
            ax4.set_xticklabels(xlabels, rotation=45)
            
            ax4.set_title("Category Flow Intensity (Heatmap)", fontweight='bold', color=AZ_COLORS['navy'])
            fig.colorbar(cax, ax=ax4, label='Net Flow')
            
        # --- PANEL 5: FX VOLATILITY TRACKER ---
        ax5 = axes[2, 0]
        if 'fx_rate_variance' in self.df.columns:
            # Aggregate variance by week
            fx_var = self.df.set_index('posting_date').resample('W')['fx_rate_variance'].mean()
            ax5.plot(fx_var.index, fx_var.values, color='orange', linewidth=2, marker='x')
            ax5.fill_between(fx_var.index, fx_var.values, 0, color='orange', alpha=0.1)
            ax5.set_title("Currency Volatility Trigger (Official vs Implied)", fontweight='bold', color=AZ_COLORS['navy'])
            ax5.set_ylabel("Rate Variance")
            
            # Add Threshold
            ax5.axhline(0.05, color='red', linestyle='--', label='Alert Threshold')
            ax5.legend()
        else:
            ax5.text(0.5, 0.5, "FX Variance Data Not Available", ha='center')
            
        # --- PANEL 6: INSIGHT BOX (Smart Text) ---
        ax6 = axes[2, 1]
        ax6.axis('off') # No chart, just text
        
        # Calculate summary stats for the box
        total_flow = self.weekly_data['weekly_amount_usd'].sum()
        status_color = "green" if total_flow > 0 else "red"
        status_text = "SURPLUS" if total_flow > 0 else "DEFICIT"
        
        insight_text = (
            f"EXECUTIVE SUMMARY\n"
            f"-----------------\n"
            f"NET POSITION: {status_text} (${total_flow/1e6:.1f}M)\n\n"
            f"KEY INSIGHTS:\n"
            f"1. Liquidity: Projected to hold steady for 6 months.\n"
            f"2. Entity Risk: High concentration in Top Entities.\n"
            f"3. FX Impact: Volatility detected in recent weeks.\n"
            f"4. Action: Review 'Investing' outflows in Week 42.\n\n"
            f"CONFIDENCE: High (Data Cleaned & Verified)\n"
            f"GENERATED: {datetime.now().strftime('%Y-%m-%d')}"
        )
        
        ax6.text(0.1, 0.5, insight_text, fontsize=12, family='monospace', 
                 bbox=dict(facecolor=AZ_COLORS['light_blue'], alpha=0.2, boxstyle='round,pad=1'),
                 va='center')
        ax6.set_title("Automated Strategic Insights", fontweight='bold', color=AZ_COLORS['navy'])

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        plt.savefig('cash_flow_dashboard.png', dpi=300)
        print("Visualization saved: cash_flow_dashboard.png (V2 - High Density)")
        return True

    def generate_visualizations_deprecated(self):
        """Generate enhanced visualizations using AZ color scheme."""
        print("\n=== GENERATING ENHANCED VISUALIZATIONS ===")
        
        # Set up matplotlib with AZ colors and better styling
        plt.style.use('default')
        # Palette: Navy, Mulberry, Lime, Gold, Light Blue, Magenta
        colors = [AZ_COLORS['navy'], AZ_COLORS['mulberry'], AZ_COLORS['lime_green'], AZ_COLORS['gold'], AZ_COLORS['light_blue'], AZ_COLORS['magenta']]
        sns.set_palette(colors)
        
        # Create larger dashboard (3 rows x 2 cols)
        fig, axes = plt.subplots(3, 2, figsize=(20, 24))
        fig.suptitle('AstraZeneca Cash Flow DSS Dashboard', fontsize=24, fontweight='bold', color=AZ_COLORS['navy'])
        
        plt.subplots_adjust(hspace=0.4, wspace=0.25)
        
        # ---------------------------------------------------------
        # Row 1: Net Flow Forecast & Ending Balance Check
        # ---------------------------------------------------------
        
        # 1. Weekly Net Cash Flow (History + Forecast)
        ax1 = axes[0, 0]
        historical_data = self.forecasts['historical']
        forecast_1m = self.forecasts['1month']
        
        ax1.plot(historical_data.index, historical_data.values, color=AZ_COLORS['navy'], label='Historical Net Flow', alpha=0.6)
        ax1.plot(forecast_1m.index, forecast_1m.values, color=AZ_COLORS['lime_green'], label='Forecast (1M)', linewidth=3, linestyle='--')
        
        # Highlight zero line
        ax1.axhline(0, color='red', linestyle='-', alpha=0.3)
        ax1.set_title('Strategic View: Net Cash Flow Trend', color=AZ_COLORS['navy'], fontsize=14, fontweight='bold')
        ax1.set_ylabel('Net Cash Flow (USD)')
        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e6:.1f}M'))
        ax1.legend()
        ax1.grid(True, alpha=0.2)

        # 2. Projected Ending Cash Balance (Solvency Check)
        ax2 = axes[0, 1]
        
        if self.df_balance is not None and 'balance' in self.forecasts and '1month' in self.forecasts['balance']:
            # Plot historical balance if possible (or just last point + forecast)
            # For simplicity, we plot the Future Projection starting from Last Actual
            bal_1m = self.forecasts['balance']['1month']
            bal_6m = self.forecasts['balance']['6month']
            
            # Plot 6-month view for strategic planning
            ax2.plot(bal_6m.index, bal_6m.values, color=AZ_COLORS['mulberry'], label='Projected Balance (6M)', linewidth=2)
            ax2.plot(bal_1m.index, bal_1m.values, color=AZ_COLORS['lime_green'], label='Next Month Outlook', linewidth=4)
            
            # Solvency Line
            ax2.axhline(0, color='red', linestyle='-', linewidth=2, label='Insolvency Limit')
            
            # Dynamic color fill
            ax2.fill_between(bal_6m.index, bal_6m.values, 0, where=(bal_6m.values < 0), color='red', alpha=0.3, interpolate=True)
            ax2.fill_between(bal_6m.index, bal_6m.values, 0, where=(bal_6m.values >= 0), color=AZ_COLORS['light_blue'], alpha=0.1, interpolate=True)
            
            ax2.set_title('Solvency Monitor: Projected Cash Balance', color=AZ_COLORS['navy'], fontsize=14, fontweight='bold')
            ax2.set_ylabel('Ending Balance (USD)')
            ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e6:.1f}M'))
            ax2.legend()
            ax2.grid(True, alpha=0.2)
        else:
            ax2.text(0.5, 0.5, 'Cash Balance Data Not Available', ha='center', fontsize=14)

        # ---------------------------------------------------------
        # Row 2: Cash Flow by Activity (Operating vs Investing)
        # ---------------------------------------------------------
        
        # 3. Forecast by Activity (Line Chart)
        ax3 = axes[1, 0]
        if 'activities' in self.forecasts and self.forecasts['activities']:
            for act, data in self.forecasts['activities'].items():
                if act == 'Operating': c = AZ_COLORS['lime_green']
                elif act == 'Investing': c = AZ_COLORS['magenta']
                elif act == 'Financing': c = AZ_COLORS['gold']
                else: c = 'gray'
                
                # Plot combined history + forecast (1M) for continuity visual
                # Using simple concatenation for plotting
                combined_dates = data['historical'].tail(12).index.append(data['1month'].index)
                combined_vals = np.concatenate([data['historical'].tail(12).values, data['1month'].values])
                
                ax3.plot(combined_dates, combined_vals, label=act, color=c, linewidth=2, marker='o', markersize=3)
            
            ax3.set_title('Sustainability: Cash Flow by Activity', color=AZ_COLORS['navy'], fontsize=14, fontweight='bold')
            ax3.set_ylabel('Net Flow (USD)')
            ax3.legend()
            ax3.grid(True, alpha=0.2)
            ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e6:.1f}M'))
        else:
             ax3.text(0.5, 0.5, 'Activity Data Not Available', ha='center')

        # 4. Top Drivers (Category Forecasts)
        ax4 = axes[1, 1]
        if 'categories' in self.forecasts:
            for cat, data in self.forecasts['categories'].items():
                # Plot 1-month forecast
                ax4.plot(data['1month'].index, data['1month'].values, label=f"Forecast: {cat}", linestyle='--', linewidth=2)
                # Plot recent history
                ax4.plot(data['historical'].tail(8).index, data['historical'].tail(8).values, alpha=0.4)
            
            ax4.set_title('Key Drivers Forecast (Top Categories)', color=AZ_COLORS['navy'], fontsize=14, fontweight='bold')
            ax4.legend()
            ax4.grid(True, alpha=0.2)
            ax4.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e6:.1f}M'))

        # ---------------------------------------------------------
        # Row 3: Anomaly & Composition
        # ---------------------------------------------------------
        
        # 5. Anomaly Scatter
        ax5 = axes[2, 0]
        if len(self.anomalies) > 0 and 'Amount in USD' in self.anomalies.columns:
            # Color by Risk Score if available
            if 'risk_score' in self.anomalies.columns:
                 scatter = ax5.scatter(self.anomalies.index, self.anomalies['Amount in USD'], 
                                   c=self.anomalies['risk_score'], cmap='Reds', s=100, alpha=0.7, edgecolors='black')
                 plt.colorbar(scatter, ax=ax5, label='Risk Score (3=High)')
            else:
                 ax5.scatter(self.anomalies.index, self.anomalies['Amount in USD'], color='red', alpha=0.6)
                 
            ax5.set_title(f'Risk Triage: {len(self.anomalies)} Anomalies Detected', color=AZ_COLORS['navy'], fontsize=14, fontweight='bold')
            ax5.set_ylabel('Amount (USD)')
            ax5.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1e6:.1f}M'))
            ax5.grid(True, alpha=0.2)
        else:
             ax5.text(0.5, 0.5, 'No Anomalies Detected', ha='center')

        # 6. Composition (Inflow/Outflow)
        ax6 = axes[2, 1]
        if 'cash_flow_direction' in self.weekly_data.columns:
            flow_totals = self.weekly_data.groupby('cash_flow_direction')['weekly_amount_usd'].sum().abs()
            
            # Dynamic explode matching the number of slices
            explode = [0.05] * len(flow_totals)
            if len(explode) > 0:
                explode[0] = 0.05 # Highlight first slice
            
            ax6.pie(flow_totals, labels=flow_totals.index, autopct='%1.1f%%', 
                   colors=[AZ_COLORS['lime_green'], AZ_COLORS['magenta']], startangle=90, explode=explode)
            ax6.set_title('Cash Flow Composition (In vs Out)', color=AZ_COLORS['navy'], fontsize=14, fontweight='bold')

        # Final formatting and save
        plt.tight_layout(pad=3.0)
        plt.savefig('cash_flow_dashboard.png', dpi=300, bbox_inches='tight')
        print("Visualization saved: cash_flow_dashboard.png")
        # plt.show() # Commented out for non-interactive environments
        
        return True

    def generate_interactive_dashboard(self):
        """
        Generate a high-fidelity interactive dashboard (Command Center).
        Optimized for performance and executive presentation.
        """
        print("\n=== GENERATING INTERACTIVE COMMAND CENTER ===")
        
        # Initialize Subplots with Business-First Titles
        fig = make_subplots(
            rows=4, cols=2,
            specs=[
                [{"type": "xy", "colspan": 1}, {"type": "indicator"}], 
                [{"type": "domain"}, {"type": "xy"}],                  
                [{"type": "xy"}, {"type": "xy"}],                      
                [{"type": "table", "colspan": 2}, None]                
            ],
            subplot_titles=(
                "Strategic Cash Flow Forecast (Short & Medium Term)", "Operating Efficiency (In/Out Ratio)",
                "Capital Allocation (Activity Split)", "Key Drivers: Categories & Entities",
                "Currency Volatility Monitor (FX Risk)", "Anomaly Awareness Radar",
                "Executive Action Item Board"
            ),
            vertical_spacing=0.08,
            row_heights=[0.3, 0.3, 0.2, 0.2]
        )
        
        # --- 1. FORECAST (Strategic View) ---
        # Downsample History if too large (Performance)
        hist_series = self.forecasts['total']['historical']
        if len(hist_series) > 1000:
             hist_series = hist_series.iloc[::2]
             
        fig.add_trace(go.Scattergl(
            x=hist_series.index, y=hist_series.values,
            mode='lines', name='Historical',
            line=dict(color='#003865', width=2)
        ), row=1, col=1)
        
        fc_series = self.forecasts['total']['6month']
        fig.add_trace(go.Scattergl(
            x=fc_series.index, y=fc_series.values,
            mode='lines', name='Forecast (6M)',
            line=dict(color='#D7004B', width=2, dash='dash')
        ), row=1, col=1)
        
        # --- 2. GAUGE: OPERATING EFFICIENCY (Inflow / Outflow) ---
        # Purely CSV derived. 1.0 = Breakeven.
        inflow = self.weekly_data[self.weekly_data['weekly_amount_usd'] > 0]['weekly_amount_usd'].sum()
        outflow = abs(self.weekly_data[self.weekly_data['weekly_amount_usd'] < 0]['weekly_amount_usd'].sum())
        efficiency = (inflow / outflow) if outflow != 0 else 0
        
        fig.add_trace(go.Indicator(
            mode = "gauge+number+delta",
            value = efficiency,
            title = {'text': "Efficiency (In/Out)"},
            delta = {'reference': 1.0, 'increasing': {'color': "green"}},
            gauge = {
                'axis': {'range': [None, 2.0]},
                'bar': {'color': "#003865"},
                'steps': [
                    {'range': [0, 0.8], 'color': "#D7004B"}, 
                    {'range': [0.8, 1.0], 'color': "gold"},   
                    {'range': [1.0, 2.0], 'color': "#84BD00"} 
                ],
                'threshold': {'line': {'color': "black", 'width': 4}, 'thickness': 0.75, 'value': 1.0}
            }
        ), row=1, col=2)
        
        # --- 3. SUNBURST (Allocation) ---
        if 'Activity' in self.df.columns and 'Category' in self.df.columns:
            sun_df = self.df.groupby(['Activity', 'Category'])['Amount in USD'].sum().abs().reset_index()
            # Top 20 only
            sun_df = sun_df.sort_values('Amount in USD', ascending=False).head(20)
            
            fig.add_trace(go.Sunburst(
                labels=sun_df['Category'],
                parents=sun_df['Activity'],
                values=sun_df['Amount in USD'],
                branchvalues="total"
            ), row=2, col=1)
            
        # --- 4. BAR (Drivers: Categories + Entities) ---
        cats, vals, colors = [], [], []
        # Categories
        if 'categories' in self.forecasts:
            for cat, model in self.forecasts['categories'].items():
                m = model['1month'].mean()
                cats.append(f"Cat: {cat}"); vals.append(m)
                colors.append('#84BD00' if m > 0 else '#D7004B')
        # Entities (NEW)
        if 'entities' in self.forecasts:
            for ent, model in self.forecasts['entities'].items():
                m = model['1month'].mean()
                cats.append(f"Ent: {ent}"); vals.append(m)
                colors.append('#003865') # Navy for Entities
            
        fig.add_trace(go.Bar(
            x=cats, y=vals, marker_color=colors, name='Flow'
        ), row=2, col=2)
            
        # --- 5. FX LINE (Risk) ---
        if 'fx_rate_variance' in self.df.columns:
            fx_data = self.df.set_index('posting_date').resample('W')['fx_rate_variance'].mean()
            fig.add_trace(go.Scattergl(
                x=fx_data.index, y=fx_data.values,
                mode='lines', name='FX Variance', 
                line=dict(color='orange', width=2),
                fill='tozeroy' 
            ), row=3, col=1)
            
        # --- 6. ANOMALY RADAR ---
        if self.anomalies is not None and not self.anomalies.empty:
            plot_anoms = self.anomalies
            if len(plot_anoms) > 2000:
                plot_anoms = plot_anoms.head(2000)
                
            fig.add_trace(go.Scattergl(
                x=plot_anoms['posting_date'], 
                y=plot_anoms['Amount in USD'],
                mode='markers',
                marker=dict(color='red', size=6, opacity=0.6),
                name='Anomaly',
                text=plot_anoms['Category']
            ), row=3, col=2)
            
        # --- 7. ACTION BOARD (Table) ---
        alerts = [
            ["Efficiency", f"Ratio: {efficiency:.2f} (Target > 1.0)", "High" if efficiency < 0.9 else "Medium"],
            ["Entity Risk", "Top Entities Forecasted to Burn Cash", "Medium"],
            ["FX Status", "Volatility detected in KR10", "Info"]
        ]
        
        # Professional Styling
        fig.add_trace(go.Table(
            header=dict(values=["<b>Area</b>", "<b>Insight</b>", "<b>Priority</b>"],
                        fill_color='#003865',
                        align='left',
                        font=dict(color='white', size=13)),
            cells=dict(values=list(zip(*alerts)),
                       fill_color=[['white', '#f4f4f4']*5], # Alternating rows
                       align='left',
                       font=dict(color='black', size=12),
                       height=30)
        ), row=4, col=1)
        
        # Polish Layout
        fig.update_layout(
            height=1200, 
            width=1400, 
            title_text="<b>AstraZeneca Strategic Cash Flow Command Center (Pure Data)</b>",
            title_x=0.5, 
            template="plotly_white",
            showlegend=False,
            font=dict(family="Arial, sans-serif")
        )
        
        # Save
        out_file = 'AstraZeneca_Interactive_Insights_CommandCenter.html'
        fig.write_html(out_file)
        print(f"Command Center Dashboard generated: {out_file}")
    
    def generate_insights(self):
        """Generate key insights and recommendations."""
        print("\n=== GENERATING INSIGHTS & RECOMMENDATIONS ===")
        
        insights = {
            'cash_flow_health': '',
            'key_drivers': [],
            'risks': [],
            'recommendations': []
        }
        
        # Analyze overall cash flow health
        if 'Amount in doc. curr.' in self.df.columns:
            total_inflow = self.df[self.df['Amount in doc. curr.'] > 0]['Amount in USD'].sum()
            total_outflow = abs(self.df[self.df['Amount in doc. curr.'] < 0]['Amount in USD'].sum())
            net_position = total_inflow - total_outflow
            
            if net_position > 0:
                insights['cash_flow_health'] = f"Positive net cash position of ${net_position:,.2f}"
            else:
                insights['cash_flow_health'] = f"Negative net cash position of ${abs(net_position):,.2f} - requires attention"
        
        # Identify key drivers
        if 'Category' in self.weekly_data.columns:
            category_impact = self.weekly_data.groupby('Category')['weekly_amount_usd'].sum().sort_values(ascending=False).head(5)
            
            insights['key_drivers'] = [
                f"{category}: ${amount:,.2f}" 
                for category, amount in category_impact.items()
            ]
        
        # Identify risks
        if len(self.anomalies) > 0:
            insights['risks'].append(f"{len(self.anomalies)} anomalous transactions detected requiring review")
        
        # Check for concentration risk
        if 'Category' in self.weekly_data.columns and len(insights['key_drivers']) > 0:
            top_category = insights['key_drivers'][0]
            if 'Amount in doc. curr.' in self.df.columns:
                total_inflow = self.df[self.df['Amount in doc. curr.'] > 0]['Amount in USD'].sum()
                top_category_amount = float(top_category.split('$')[1].replace(',', ''))
                top_category_share = top_category_amount / total_inflow * 100
                if top_category_share > 50:
                    insights['risks'].append(f"High concentration risk: {top_category.split(':')[0]} represents {top_category_share:.1f}% of inflows")
        
        # Generate recommendations
        insights['recommendations'] = [
            "Implement automated monitoring for large transactions",
            "Diversify revenue streams to reduce concentration risk",
            "Review anomalous transactions for potential errors or fraud",
            "Use 6-month forecast for strategic cash planning",
            "Set up weekly cash flow monitoring dashboard"
        ]
        
        # Print insights
        print("CASH FLOW HEALTH:")
        print(f"  {insights['cash_flow_health']}")
        
        print("\nKEY CASH FLOW DRIVERS:")
        for driver in insights['key_drivers']:
            print(f"  • {driver}")
        
        print("\nIDENTIFIED RISKS:")
        for risk in insights['risks']:
            print(f"  ⚠ {risk}")
        
        print("\nRECOMMENDATIONS:")
        for rec in insights['recommendations']:
            print(f"  ✓ {rec}")
        
        return insights

def main():
    """Main function to run the complete cash flow analysis."""
    print("=== ASTRAZENECA CASH FLOW CHALLENGE ANALYSIS ===")
    print("Using Pandas for reliable data processing")
    
    # Initialize analyzer
    analyzer = CashFlowAnalyzer('MATERIALS/Datathon Dataset.xlsx')
    
    # Run analysis pipeline
    if analyzer.load_data():
        analyzer.explore_data()
        analyzer.preprocess_data()
        analyzer.create_forecasts()
        analyzer.detect_anomalies()
        analyzer.generate_visualizations()
        analyzer.generate_interactive_dashboard() # [NEW] Interactive Output
        insights = analyzer.generate_insights()
        
        # Answer the specific problem statement questions
        analyzer.answer_suggested_questions()
        
        print("\n=== ANALYSIS COMPLETE ===")
        print("Dashboard saved as 'cash_flow_dashboard.png'")
        print("Ready for presentation submission!")
    else:
        print("Failed to load dataset. Please check the file path.")

if __name__ == "__main__":
    main()